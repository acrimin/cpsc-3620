{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## <center>Introduction to Hadoop Distributed File System (HDFS)</center>\n",
    "### <center> Linh B. Ngo </center>\n",
    "### <center> CPSC 3620 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <center> Overview </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- 2002: Doug Cutting and Mike Carafella started a project to build an open-source search engine called Nutch. A component of this project was a web crawler that can crawl and index the Internet.\n",
    "- 2003: Google released a research paper on its in-house data storage system called [Google File System](http://dl.acm.org/citation.cfm?id=945450) (GFS).\n",
    "- 2004: Google released another research paper on the programming approach to process data stored on GFS, called [MapReduce](http://dl.acm.org/citation.cfm?id=1327492)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- 2005: Cutting and Carafelle rebuilt the underlying file management system and processing framework of Nutch based on the architectural design of Google's GFS and MapReduce.\n",
    "- 2006: The adaptations of Google's GFS and MapReduce are converted into a single open source project called Hadoop, which is sponsored by Yahoo and led by Doug Cutting.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- 2007: Yahoo maintains a 1000-node production cluster.\n",
    "- 2008: Hadoop becomes the platform of Yahoo's web index. Hadoop wins record for world fastest system to sort one terabyte of data (209 seconds using a **910-node** cluster). Hadoop becomes a top-level open source project of Apache Foundation. First Hadoop commercial distributor led by a former Google employee, Cloudera, is founded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- 2009: Hadoop sorts one terabyte of data in 62 seconds and one petabyte of data in 16.25 hours using a **3800-node** cluster. Second Hadoop commercial distributor, MapR, is formed.\n",
    "- 2011: Yahoo spins off its own Hadoop comnmercial distributor, Hortonworks.\n",
    "- 2012: Apache Hadoop 1.0 is released."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Corresponding Component Names**\n",
    "- Google File Systems (GFS): Hadoop Distributed File System (HDFS)\n",
    "- Google MapReduce: Hadoop MapReduce\n",
    "- Google BigTable: Apache HBase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Corresponding Component Names**\n",
    "- GFS Master: NameNode\n",
    "- Chunkserver: DataNode\n",
    "- Chunks: Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Apache Hadoop Project**\n",
    "- Hadoop Distributed File System\n",
    "- YARN (Yet Another Resource Negotiator)\n",
    "- Hadoop MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <center> Hadoop Distributed File Systems </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Design Assumptions and Goals**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Hardware failure is the norm rather than the exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Streaming data access\n",
    "    - Not for general purpose applications \n",
    "    - For batch processing rather than interactive use \n",
    "    - For high throughput of data access rather than low latency of data access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Large data sets (terabytes in size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Simple coherency model (write once read many)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Moving computation is cheaper than moving data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Portability across heterogeneous hardware and software platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**HDFS Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div>\n",
    "    <div style=\"max-width:200px;float:left\">\n",
    "        <p> Master/Slave Architecture:\n",
    "        <p> - Master: NameNode\n",
    "        <p> - Workers: Data Node\n",
    "    </div>\n",
    "    <div style=\"max-width:700px;float:right\">\n",
    "        <center> \n",
    "            <img src=\"pictures/hdfsarchitecture.png\" width=\"700\"/>\n",
    "        </center>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div>\n",
    "    <div style=\"max-width:200px;float:left\">\n",
    "        <p> NameNode: \n",
    "        <p> - manages the file system namespace \n",
    "        <p> - regulates access to files by clients\n",
    "        <p> - executes file system namespace operations \n",
    "        <p> - determines the mapping of blocks to DataNodes\n",
    "    </div>\n",
    "    <div style=\"max-width:700px;float:right\">\n",
    "        <center> \n",
    "            <img src=\"pictures/hdfsarchitecture.png\" width=\"700\"/>\n",
    "        </center>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div>\n",
    "    <div style=\"max-width:200px;float:left\">\n",
    "        <p> DataNode:\n",
    "        <p> - one per node in the cluster\n",
    "        <p> - manages storage attached to the node\n",
    "        <p> - serves read and write requests clients\n",
    "    </div>\n",
    "    <div style=\"max-width:700px;float:right\">\n",
    "        <center> \n",
    "            <img src=\"pictures/hdfsarchitecture.png\" width=\"700\"/>\n",
    "        </center>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div>\n",
    "    <div style=\"max-width:200px;float:left\">\n",
    "        <p> Files and Directories:\n",
    "        <p> - HDFS file system namespace is exposed \n",
    "        <p> - Operations include opening, closing, and renaming files and directories.\n",
    "        <p> - HDFS allows user data to be stored in files\n",
    "    </div>\n",
    "    <div style=\"max-width:700px;float:right\">\n",
    "        <center> \n",
    "            <img src=\"pictures/hdfsarchitecture.png\" width=\"700\"/>\n",
    "        </center>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div>\n",
    "    <div style=\"max-width:200px;float:left\">\n",
    "        <p> Files and Directories:\n",
    "        <p> - Internally, a file is split into one or more blocks\n",
    "        <p> - Blocks have equal and fixed size and are stored in a set of DataNodes\n",
    "        <p> - NameNode determines the mapping of blocks to DataNodes. \n",
    "        <p> - DataNodes perform block creation, deletion, and replication upon instruction from the NameNode.\n",
    "    </div>\n",
    "    <div style=\"max-width:700px;float:right\">\n",
    "        <center> \n",
    "            <img src=\"pictures/hdfsarchitecture.png\" width=\"700\"/>\n",
    "        </center>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div>\n",
    "    <div style=\"max-width:200px;float:left\">\n",
    "        <p> Data Replication:\n",
    "        <p> - \"Failure is the norm and not the exception\"\n",
    "        <p> - Blocks are replicated for fault tolerance. \n",
    "\n",
    "    </div>\n",
    "    <div style=\"max-width:700px;float:right\">\n",
    "        <center> \n",
    "            <img src=\"pictures/hdfsdatanodes.png\" width=\"600\"/>\n",
    "        </center>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div>\n",
    "    <div style=\"max-width:200px;float:left\">\n",
    "        <p> Data Replication:\n",
    "        <p> - The block size and replication factor are configurable per file.\n",
    "        <p> - NameNode makes all decisions regarding replication of blocks.         \n",
    "    </div>\n",
    "    <div style=\"max-width:700px;float:right\">\n",
    "        <center> \n",
    "            <img src=\"pictures/hdfsdatanodes.png\" width=\"600\"/>\n",
    "        </center>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Replica Placement**\n",
    "- Placement of replicas is critical to HDFS reliability and performance. \n",
    "- The purpose of a rack-aware replica placement policy is to improve data reliability, availability, and network bandwidth utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Replica Placement: Hardware Settings**\n",
    "- Cluster of computers that commonly spread across many racks. \n",
    "- Racks are connected via network switches. \n",
    "- In most cases, network bandwidth between machines in the same rack is greater than network bandwidth between machines in different racks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Placement Policy: Simple and non-optimal**\n",
    "- place replicas on unique racks. \n",
    "- Advantage: \n",
    "    - prevents losing data when an entire rack fails \n",
    "    - allows use of bandwidth from multiple racks when reading data\n",
    "    - evenly distributes replicas in the cluster \n",
    "- Disadvantage:\n",
    "    - increases the cost of writes because a write needs to transfer blocks to multiple racks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Placement Policy: HDFS default policy**\n",
    "- Default replication factor: 3\n",
    "    - First replica on one node in the local rack, \n",
    "    - Second replica on a different node in the local rack, \n",
    "    - Third replica on a different node in a different rack.\n",
    "- Reduces the inter-rack write traffic \n",
    "- Does not loose data reliablity and availability guarantees \n",
    "- Does not evenly distribute across the racks:\n",
    "    - One third of replicas are on one node, \n",
    "    - two thirds of replicas are on one rack, and \n",
    "    - the other third are evenly distributed across the remaining racks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Placement Policy: HDFS default policy**\n",
    "- Replication factor greater than 3:\n",
    "    - Random determination of placement\n",
    "    - Rack limit: $\\frac{replicas - 1}{racks + 2}$\n",
    "\n",
    "- DataNodes are not allowed to have multiple replicas of the same block\n",
    "- Maximum number of replicas created is the total number of DataNodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <center> Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**HDFS Writes**\n",
    "- Staging\n",
    "- Pipelining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Staging**\n",
    "- HDFS client caches the file data into a local buffer\n",
    "- Application writes are transparently redirected to this local buffer\n",
    "- When the local file accumulates data worth over one chunk size, the client contacts the NameNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- NameNode\n",
    "    - inserts the file name into the file system hierarchy and allocates a data block for it. \n",
    "    - responds to the client request with the identity of the DataNode and the destination data block. \n",
    "- Client flushes the chunk of data from the local buffer to the specified DataNode. \n",
    "- When a file is closed:\n",
    "    - the remaining un-flushed data in the local buffer is transferred to the DataNode. \n",
    "    - The client then tells the NameNode that the file is closed. \n",
    "    - NameNode commits the file creation operation into a persistent store. \n",
    "    - *If the NameNode dies before the file is closed, the file is lost.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Pipelining**\n",
    "- Replication factor of three. \n",
    "- Pipelining the writing and replicating process of individual chunks of each block\n",
    "- List of DataNodes comes from NameNode\n",
    "- The client then flushes the data chunk to the first DataNode. \n",
    "- For each DataNode:\n",
    "    - receiving the data in small portions, \n",
    "    - writes each portion to its local repository, and\n",
    "    - transfers that portion to the second DataNode in the list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### HDFS Reads\n",
    "- Parallel reads on blocks\n",
    "- Applications take advantage of parallel I/O via MapReduce programming approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <center> Hadoop 1.0: JobTracker and TaskTracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center> \n",
    "    <img src=\"pictures/hadoopjobtracker.png\" width=\"900\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**First resource manager/job controller on HDFS**\n",
    "- Specifically tied to the MapReduce programming model\n",
    "- Data-aware (via NameNode interactions) \n",
    "- Task-aware (via TaskTracker interactions)\n",
    "- Rely on heartbeat messages, which also contain slot availability information from TaskTrackers:\n",
    "    - System health\n",
    "    - Execution availability\n",
    "    - Job progress\n",
    "- Manage job execution process\n",
    "- Rerun tasks lost due to node failures\n",
    "– Speculative execution\n",
    "- Single point of failure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Execution Progress**\n",
    "- MapReduce job is submitted to the JobTracker\n",
    "- JobTracker determines data location via NameNode\n",
    "- JobTracker locates TaskTracker nodes\n",
    "    - With available slots\n",
    "    -Nearest to the data location\n",
    "- JobTracker sends job (tasks of job: Map, Reduce, Shuffle) to the selected TaskTrackers\n",
    "- TaskTrackers spawn separate JVMs to run tasks (Map, Reduce, Shuffle) for the job\n",
    "- TaskTrackers maintain detailed log for progress, output, and exit codes\n",
    "- Heartbeat messages are sent frequently from TaskTrackers to JobTracker\n",
    "    - Alive\n",
    "    - Slot availability\n",
    "- If no heartbeat is received within a preconfigured duration, the tasks assigned to that TaskTracker are resubmitted to another TaskTracker\n",
    "- JobTracker updates status report when job is completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <center> Hadoop 2.0: YARN (Yet Another Resource Negotiator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center> \n",
    "    <img src=\"pictures/hadoopyarn.png\" width=\"700\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Conceptual Design/Differences**\n",
    "- Pure scheduler: limited to arbitrating available resources in the system\n",
    "- Pluggable scheduler: multiple scheduling algorithms\n",
    "- Job management is handle by ApplicationMaster\n",
    "- Resource Model:\n",
    "    - Resource-name (hostname, rackname)\n",
    "    - Memory (MB)\n",
    "    - CPU (cores)\n",
    "- ResourceRequest:\n",
    "    - resource-name, priority, resource-requirement, number-of-containers\n",
    "    - Container: the resource allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div>\n",
    "    <div style=\"max-width:200px;float:left\">\n",
    "        <p> 1. A client program submits the application        \n",
    "    </div>\n",
    "    <div style=\"max-width:700px;float:right\">\n",
    "        <center> \n",
    "            <img src=\"pictures/yarnflow.png\" width=\"700\"/>\n",
    "        </center>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div>\n",
    "    <div style=\"max-width:200px;float:left\">\n",
    "        <p> 2. ResourceManager negotiates a container to start the ApplicationMaster and then launches the ApplicationMaster\n",
    "    </div>\n",
    "    <div style=\"max-width:700px;float:right\">\n",
    "        <center> \n",
    "            <img src=\"pictures/yarnflow.png\" width=\"700\"/>\n",
    "        </center>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div>\n",
    "    <div style=\"max-width:200px;float:left\">\n",
    "        <p> 3. The ApplicationMaster, on boot-up, registers with the ResourceManager. This allows the client to query the ResourceManager for details to directly interact with its ApplicationMaster\n",
    "    </div>\n",
    "    <div style=\"max-width:700px;float:right\">\n",
    "        <center> \n",
    "            <img src=\"pictures/yarnflow.png\" width=\"700\"/>\n",
    "        </center>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div>\n",
    "    <div style=\"max-width:200px;float:left\">\n",
    "        <p> 4. ApplicationMaster negotiates resource containers via the resource-request protocol \n",
    "    </div>\n",
    "    <div style=\"max-width:700px;float:right\">\n",
    "        <center> \n",
    "            <img src=\"pictures/yarnflow.png\" width=\"700\"/>\n",
    "        </center>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div>\n",
    "    <div style=\"max-width:200px;float:left\">\n",
    "        <p> 5. After successful allocations, the ApplicationMaster launches the container by providing the container launch specification to the NodeManager. This includes command line to launch, environment variables, local resources (jars, shared-\n",
    "objects, ...), and security-related tokens\n",
    "    </div>\n",
    "    <div style=\"max-width:700px;float:right\">\n",
    "        <center> \n",
    "            <img src=\"pictures/yarnflow.png\" width=\"700\"/>\n",
    "        </center>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div>\n",
    "    <div style=\"max-width:200px;float:left\">\n",
    "        <p> 6. The application code executing within the container then provides logging info back to its ApplicationMaster via an application-specific protocol        \n",
    "    </div>\n",
    "    <div style=\"max-width:700px;float:right\">\n",
    "        <center> \n",
    "            <img src=\"pictures/yarnflow.png\" width=\"700\"/>\n",
    "        </center>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div>\n",
    "    <div style=\"max-width:200px;float:left\">        \n",
    "        <p> 7. During the application execution, the client that submitted the program communicates directly with the ApplicationMaster to get status, progress, updates via an application-specific protocol\n",
    "    </div>\n",
    "    <div style=\"max-width:700px;float:right\">\n",
    "        <center> \n",
    "            <img src=\"pictures/yarnflow.png\" width=\"700\"/>\n",
    "        </center>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div>\n",
    "    <div style=\"max-width:200px;float:left\">\n",
    "        <p> 8. Upon completion, the ApplicationMaster deregisters with the ResourceManager and shuts down, allowing its own container to be repurposed\n",
    "    </div>\n",
    "    <div style=\"max-width:700px;float:right\">\n",
    "        <center> \n",
    "            <img src=\"pictures/yarnflow.png\" width=\"700\"/>\n",
    "        </center>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <center> References </center>\n",
    "- https://hadoop.apache.org/docs/r3.0.0-alpha1/\n",
    "- http://hortonworks.com/blog/apache-hadoop-yarn-concepts-and-applications/"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Anaconda 2.5.0 (Python 3)",
   "language": "python",
   "name": "anaconda_2.5.0_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
