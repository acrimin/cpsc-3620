{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>MPIIO - Distributed File Systems</center>\n",
    "### <center> Linh B. Ngo </center>\n",
    "### <center> CPSC 3620 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <center> Common Ways of Doing I/O in Parallel Programs </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**All processes send data to rank 0, and 0 writes it to the file**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/mpiio1.png\" width=\"700\"/> \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Pros:\n",
    "    - parallel machine may support I/O from only one process (e.g., no common file system)\n",
    "    - some I/O libraries (e.g. HDF-4, NetCDF) not parallel\n",
    "    - resulting single file is handy for ftp, mv\n",
    "    - big blocks improve performance\n",
    "    - short distance from original, serial code\n",
    "- Cons:\n",
    "    - lack of parallelism limits scalability, performance (single node bottleneck)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Each process writes to a separate file**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/mpiio2.png\" width=\"700\"/> \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Pros: \n",
    "    - parallelism, high performance\n",
    "- Cons:  \n",
    "    - lots of small files to manage\n",
    "    - difficult to read back data from different number of processes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <center>MPI-IO Approach</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** What is Parallel I/O? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Multiple processes of a parallel program accessing data (reading or writing) from a common file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center> <img src=\"pictures/mpiio3.png\" width=\"700\"/> \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Why Parallel I/O? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Non-parallel I/O is simple but\n",
    "    - Poor performance (single process writes to one file) or\n",
    "    - Awkward and not interoperable with other tools (each process writes a separate file)\n",
    "- Parallel I/O\n",
    "    - Provides high performance\n",
    "    - Can provide a single file that can be used with other tools (such as visualization programs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Why is MPI a good setting for Parallel I/O? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Writing is like sending a message and reading is like receiving\n",
    "- Any parallel I/O system will need a mechanism to\n",
    "    - define collective operations (MPI communicators)\n",
    "    - define noncontiguous data layout in memory and file (MPI datatypes)\n",
    "    - test completion of nonblocking operations (MPI request objects)\n",
    "- i.e., lots of MPI-like machinery\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Four stages\n",
    "    * Open File\n",
    "    * Set File View (optional)\n",
    "    * Read or Write Data\n",
    "    * Close File\n",
    "- All the complexity is hidden in setting the file view\n",
    "- Write is probably more important in practice than read\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Opening a File (C Syntax) **\n",
    "\n",
    "~~~\n",
    "int MPI_File_open(MPI_Comm comm, const char *filename, int amode, MPI_Info info, MPI_File *fh)\n",
    "~~~\n",
    "\n",
    "- amode \tFile access mode (integer)\n",
    "- info \t\tInfo object (handle)\n",
    "- fh \t\tNew file handle (handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- MPI_File_open opens the file identified by the filename filename on all processes in the comm communicator group. \n",
    "- MPI_File_open is a collective routine; all processes must provide the same value for amode, and all processes must provide filenames that reference the same file and which are textually identical. A process can open a file independently of other processes by using the MPI_COMM_SELF communicator. \n",
    "- The file handle returned, fh, can be subsequently used to access the file until the file is closed using MPI_File_close. Before calling MPI_Finalize, the user is required to close (via MPI_File_close) all files that were opened with MPI_File_open. \n",
    "- Initially, all processes view the file as a linear byte stream; that is, the etype and filetype are both MPI_BYTE. The file view can be changed via the MPI_File_set_view routine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- MPI_MODE_APPEND \n",
    "- MPI_MODE_CREATE -- Create the file if it does not exist. \n",
    "- MPI_MODE_DELETE_ON_CLOSE \n",
    "- MPI_MODE_EXCL -- Error creating a file that already exists. \n",
    "- MPI_MODE_RDONLY -- Read only. \n",
    "- MPI_MODE_RDWR -- Reading and writing. \n",
    "- MPI_MODE_SEQUENTIAL \n",
    "- MPI_MODE_WRONLY -- Write only. \n",
    "- MPI_MODE_UNIQUE_OPEN\n",
    "\n",
    "The modes MPI_MODE_RDONLY, MPI_MODE_RDWR, MPI_MODE_WRONLY, and MPI_MODE_CREATE have identical semantics to their POSIX counterparts. It is erroneous to specify MPI_MODE_CREATE in conjunction with MPI_MODE_RDONLY. Errors related to the access mode are raised in the class MPI_ERR_AMODE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Set File View (C Syntax) **\n",
    "~~~\n",
    "int MPI_File_set_view(MPI_File fh, MPI_Offset disp, MPI_Datatype etype, MPI_Datatype filetype, const char *datarep, MPI_Info info)\n",
    "~~~\n",
    "\n",
    "- disp Displacement (integer).\n",
    "- etype Elementary data type (handle).\n",
    "- filetype File type (handle). See Restrictions, below.\n",
    "- datarep Data representation (string).\n",
    "- info Info object (handle)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The MPI_File_set_view routine changes the processâ€™s view of the data in the file:\n",
    "- the beginning of the data accessible in the file through that view is set to disp\n",
    "- the type of data is set to etype; and the distribution of data to processes is set to filetype. \n",
    "- resets the independent file pointers and the shared file pointer to zero. \n",
    "- is collective across the fh; all processes in the group must pass identical values for datarep and provide an etype with an identical extent. \n",
    "- the values for disp, filetype, and info may vary. \n",
    "- he disp displacement argument specifies the position (absolute offset in bytes from the beginning of the file) where the view begins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Reading a File (C Syntax) **\n",
    "~~~\n",
    "int MPI_File_read(MPI_File fh, void *buf,  int count, MPI_Datatype datatype, MPI_Status *status)\n",
    "~~~\n",
    "\n",
    "- fh File handle (handle).\n",
    "- count Number of elements in buffer (integer).\n",
    "- datatype Data type of each buffer element (handle).\n",
    "- buf Initial address of buffer (integer).\n",
    "- status Status object (status).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Reading a File (C Syntax) **\n",
    "~~~\n",
    "int MPI_File_seek(MPI_File fh, MPI_Offset offset, int whence)\n",
    "~~~\n",
    "\n",
    "- fh File handle (handle).\n",
    "- offset File offset (integer).\n",
    "- whence Update mode (integer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "MPI_File_seek updates the individual file pointer according to whence, which could have the following possible values: \n",
    "\n",
    "- MPI_SEEK_SET - The pointer is set to offset. \n",
    "- MPI_SEEK_CUR - The pointer is set to the current pointer position plus offset. \n",
    "- MPI_SEEK_END - The pointer is set to the end of the file plus offset.\n",
    "\n",
    "The offset can be negative, which allows seeking backwards. It is erroneous to seek to a negative position in the file. The end of the file is defined to be the location of the next elementary data item immediately after the last accessed data item, even if that location is a hole.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Closing a File (C Syntax) **\n",
    "~~~\n",
    "MPI_File_close(MPI_File *fh)\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "import ipyparallel\n",
    "c=ipyparallel.Client(profile=\"mpicluster\")\n",
    "print(c.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] [3 3 3 3 3 3 3 3 3 3]\n",
      "[stdout:1] [1 1 1 1 1 1 1 1 1 1]\n",
      "[stdout:2] \n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3\n",
      " 3 3 3]\n",
      "[stdout:3] [2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "    \n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "amode = MPI.MODE_WRONLY|MPI.MODE_CREATE\n",
    "comm = MPI.COMM_WORLD\n",
    "fh = MPI.File.Open(comm, \"./datafile.contig\", amode)\n",
    "   \n",
    "buffer = np.empty(10, dtype=np.int)\n",
    "buffer[:] = rank\n",
    "print(buffer)\n",
    "\n",
    "offset = comm.Get_rank()*buffer.nbytes\n",
    "fh.Write_at_all(offset, buffer)  \n",
    "fh.Close()\n",
    "\n",
    "if (rank == 0):\n",
    "    print (np.fromfile(\"./datafile.contig\", dtype=\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] [3 3 3 3 3 3 3 3 3 3]\n",
      "[stdout:1] [1 1 1 1 1 1 1 1 1 1]\n",
      "[stdout:2] \n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0\n",
      " 1 2 3 3 0 0 0 0 0 1 2 3 0 0 0 0 0 1 2 3 0 0 0 0 0 1 2 3 0 0 0 0 0 1 2 3 0\n",
      " 0 0 0]\n",
      "[stdout:3] [2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank();size = comm.Get_size();\n",
    "\n",
    "amode = MPI.MODE_WRONLY|MPI.MODE_CREATE\n",
    "fh = MPI.File.Open(comm, \"./datafile.noncontig\", amode)\n",
    "item_count = 10\n",
    "buffer = np.empty(item_count, dtype='i')\n",
    "buffer[:] = rank\n",
    "print (buffer)\n",
    "filetype = MPI.INT.Create_vector(item_count, 1, size)\n",
    "filetype.Commit()\n",
    "\n",
    "displacement = MPI.INT.Get_size()*rank\n",
    "fh.Set_view(displacement, filetype=filetype)\n",
    "\n",
    "fh.Write_all(buffer)\n",
    "filetype.Free()\n",
    "fh.Close()\n",
    "\n",
    "if (rank == 0):\n",
    "    print (np.fromfile(\"./datafile.noncontig\", dtype=\"i\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <center> Under the Covers of MPI-IO </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- MPI-IO implementation is given a lot of information in this case:\n",
    "    - Collection of processes reading data\n",
    "    - Structured description of the regions\n",
    "- Implementation has some options for how to obtain this data\n",
    "    - Noncontiguous data access optimizations\n",
    "    - Collective I/O optimizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#### <center> General Guidelines for Achieving High I/O Performance </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Buy sufficient I/O hardware for the machine\n",
    "- Use fast file systems, not NFS-mounted home directories\n",
    "- Do not perform I/O from one process only\n",
    "- Make large requests wherever possible\n",
    "- For noncontiguous requests, use derived datatypes and a single collective I/O call\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Anaconda 2.5.0 (Python 3)",
   "language": "python",
   "name": "anaconda_2.5.0_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
