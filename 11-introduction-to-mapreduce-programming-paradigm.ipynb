{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Introduction to Hadoop MapReduce </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reality of working with Big Data\n",
    "\n",
    "- Hundreds or thousands of machines to support big data\n",
    "    - Distribute data for storage (HDFS)\n",
    "    - Parallelize data computation (Hadoop MapReduce)\n",
    "    - Handle failure (HDFS and Hadoop MapReduce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MapReduce\n",
    "\n",
    "** What is “map”? **\n",
    "A function/procedure that is applied to every individual elements of a collection/list/array/…\n",
    "```\n",
    "int square(x) { return x*x;}\n",
    "map square [1,2,3,4] -> [1,4,9,16]\n",
    "```\n",
    "** What is “reduce”? **\n",
    "A function/procedure that performs an operation on a list. This operation will “fold/reduce” this list into a single value (or a smaller subset)\n",
    "```\n",
    "reduce ([1,2,3,4]) using sum -> 10\n",
    "reduce ([1,2,3,4]) using multiply -> 24\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementation of MapReduce Programming Paradigm in Hadoop MapReduce\n",
    "\n",
    "**Programmers implement:**\n",
    "\n",
    "- Map function: Take in the input data and return a key,value pair\n",
    "- Reduce function: Receive the key,value pairs from the mapper and provide a final output as a reduction operation on the pairs\n",
    "- Optional functions:\n",
    "    - Partition function: determines the distribution of mappers’ key,value pairs to the reducers\n",
    "    - Combine functions: initial reduction on the mappers to reduce network traffics\n",
    "    \n",
    "**The MapReduce Framework handles everything else**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## WordCount: The *Hello, World* of Big Data\n",
    "\n",
    "- Count how many unique words there are in a file/multiple files\n",
    "- Standard parallel programming approach:\n",
    "    - Count number of files\n",
    "    - Set number of processes\n",
    "    - Possibly setting up dynamic workload assignment\n",
    "    - A lot of data transfer\n",
    "    - Significant coding effort\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MapReduce WordCount Example\n",
    "\n",
    "<img src=\"pictures/11/wordcount01.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MapReduce WordCount Example\n",
    "\n",
    "<img src=\"pictures/11/wordcount02.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MapReduce PageRank Example 1\n",
    "\n",
    "<img src=\"pictures/11/pagerank01.png\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MapReduce PageRank Example 2\n",
    "\n",
    "<img src=\"pictures/11/pagerank02.png\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is \"everything else\"?\n",
    "\n",
    "- Scheduling\n",
    "- Data distribution\n",
    "- Synchronization\n",
    "- Error and Fault Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The cost of \"everything else\"?\n",
    "\n",
    "- All algorithms must be expressed as a combination of mapping, reducing, combining, and partitioning functions \n",
    "- No control over execution placement of mappers and reducers\n",
    "- No control over life cycle of individual mappers and reducers\n",
    "- Very limited information about which mapper handles which data block\n",
    "- Very limited information about which reducer handles which intermediate key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Additional challenge\n",
    "\n",
    "** Large scale debugging on big data programming is difficult\n",
    "\n",
    "- Functional errors are difficult to follow at large scale\n",
    "- Data-dependent errors are even more difficult to catch and fix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applications of MapReduce\n",
    "\n",
    "- Text tokenization, indexing, and search\n",
    "    - Web access log stats\n",
    "    - Inverted index construction\n",
    "    - Term-vector per host\n",
    "    - Distributed grep/sort\n",
    "- Graph creation\n",
    "    - Web link-graph reversal (Google’s PageRank)\n",
    "- Data Mining and machine learning\n",
    "    - Document clustering\t\n",
    "    - Machine learning\n",
    "    - Statistical machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Working with Hadoop MapReduce on Cypress </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1 8 8 , 1hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Jupyter notebook supports execution of Linux command inside the notebook cells. This is done by adding the **!** to the beginning of the command line. It should be noted that each command begins with a **!** will create a new bash shell and close this cell once the execution is done:\n",
    "- Full path is required\n",
    "- Temporary results and environmental variables will be lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Create a directory named **intro-to-hadoop** inside your user directory on HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -mkdir intro-to-hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Upload the three data directories, **airlines**, **movielens**, and **text** into the newly created **intro-to-hadoop** directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -put \\\n",
    "    /home/lngo/intro-to-hadoop/airlines/ \\\n",
    "    intro-to-hadoop/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -put \\\n",
    "    /home/lngo/intro-to-hadoop/movielens/ \\\n",
    "    intro-to-hadoop/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -put \\\n",
    "    /home/lngo/intro-to-hadoop/text/ \\\n",
    "    intro-to-hadoop/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge \n",
    "\n",
    "Check the health status of the directories above in HDFS using fsck:\n",
    "```\n",
    "hdfs fsck <path-to-directory> -files -blocks -locations\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 fsck intro-to-hadoop/text -files -blocks -locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Hello World of Hadoop: Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -cat intro-to-hadoop/text/gutenberg-shakespeare.txt \\\n",
    "    2>/dev/null | head -n 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -cat intro-to-hadoop/text/gutenberg-shakespeare.txt \\\n",
    "    2>/dev/null \\\n",
    "    | head -n 100 \\\n",
    "    | python /home/lngo/intro-to-hadoop/wordcountMapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -cat intro-to-hadoop/text/gutenberg-shakespeare.txt \\\n",
    "    2>/dev/null \\\n",
    "    | head -n 100 \\\n",
    "    | python /home/lngo/intro-to-hadoop/wordcountMapper.py \\\n",
    "    | sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -cat intro-to-hadoop/text/gutenberg-shakespeare.txt \\\n",
    "    2>/dev/null \\\n",
    "    | head -n 100 \\\n",
    "    | python /home/lngo/intro-to-hadoop/wordcountMapper.py \\\n",
    "    | sort \\\n",
    "    | python /home/lngo/intro-to-hadoop/wordcountReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input intro-to-hadoop/text/gutenberg-shakespeare.txt \\\n",
    "    -output intro-to-hadoop/output-wordcount \\\n",
    "    -file /home/lngo/intro-to-hadoop/wordcountMapper.py \\\n",
    "    -mapper wordcountMapper.py \\\n",
    "    -file /home/lngo/intro-to-hadoop/wordcountReducer.py \\\n",
    "    -reducer wordcountReducer.py \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -ls intro-to-hadoop/output-wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -cat intro-to-hadoop/output-wordcount/part-00000 \\\n",
    "    2>/dev/null | head -n 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Modify *wordcountMapper.py* so that punctuations and capitalization are no longer factors in determining unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Movie Ratings and Recommendation\n",
    "\n",
    "An independent movie company is looking to invest in a new movie project. With limited finance, the company wants to \n",
    "analyze the reaction of audiences, particularly toward various movie genres, in order to identify beneficial \n",
    "movie project to focus on. The company relies on data collected from a publicly available recommendation service \n",
    "by [MovieLens](http://dl.acm.org/citation.cfm?id=2827872). This \n",
    "[dataset](http://files.grouplens.org/datasets/movielens/ml-10m-README.html) contains **22884377** ratings and **586994**\n",
    " tag applications across **34208** movies. These data were created by **247753** users between January 09, 1995 and January 29, 2016. This dataset was generated on January 29, 2016. \n",
    "\n",
    "From this dataset, several analyses are possible, include the followings:\n",
    "1.   Find movies which have the highest average ratings over the years and identify the corresponding genre.\n",
    "2.   Find genres which have the highest average ratings over the years.\n",
    "3.   Find users who rate movies most frequently in order to contact them for in-depth marketing analysis.\n",
    "\n",
    "These types of analyses, which are somewhat ambiguous, demand the ability to quickly process large amount of data in \n",
    "elatively short amount of time for decision support purposes. In these situations, the sizes of the data typically \n",
    "make analysis done on a single machine impossible and analysis done using a remote storage system impractical. For \n",
    "remainder of the lessons, we will learn how HDFS provides the basis to store massive amount of data and to enable \n",
    "the programming approach to analyze these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -ls -h intro-to-hadoop/movielens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Find movies which have the highest average ratings over the years and identify the corresponding genre\n",
    "\n",
    "- Find the average ratings of all movies over the years\n",
    "- Identify the corresponding genres for each movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -ls intro-to-hadoop/movielens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -cat intro-to-hadoop/movielens/README.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -cat intro-to-hadoop/movielens/links.csv \\\n",
    "    2>/dev/null | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -cat intro-to-hadoop/movielens/movies.csv \\\n",
    "    2>/dev/null | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -cat intro-to-hadoop/movielens/ratings.csv \\\n",
    "    2>/dev/null | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -cat intro-to-hadoop/movielens/tags.csv \\\n",
    "    2>/dev/null | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "To write a MapReduce program, you have to be able to identify the necessary (Key,Value) that can contribute to the final realization of the required results. This is the reducing phase. From this (Key,Value) pair format, you will be able to develop the mapping phase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -cat intro-to-hadoop/movielens/ratings.csv \\\n",
    "    2>/dev/null | head -n 5 | python /home/lngo/intro-to-hadoop/avgRatingMapper01.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Do we really need the headers?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -cat intro-to-hadoop/movielens/ratings.csv \\\n",
    "    2>/dev/null | head -n 5 | python /home/lngo/intro-to-hadoop/avgRatingMapper02.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *The outcome is correct. Is it useful?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -cat intro-to-hadoop/movielens/ratings.csv \\\n",
    "    2>/dev/null | head -n 5 | python /home/lngo/intro-to-hadoop/avgRatingMapper03.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -cat intro-to-hadoop/movielens/ratings.csv 2>/dev/null \\\n",
    "    | head -n 5 \\\n",
    "    | python /home/lngo/intro-to-hadoop/avgRatingMapper03.py \\\n",
    "    | sort \\\n",
    "    | python /home/lngo/intro-to-hadoop/avgRatingReducer01.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-HDFS correctness test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -cat intro-to-hadoop/movielens/ratings.csv 2>/dev/null \\\n",
    "    | head -n 1000 \\\n",
    "    | python /home/lngo/intro-to-hadoop/avgRatingMapper03.py \\\n",
    "    | grep Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -cat intro-to-hadoop/movielens/ratings.csv 2>/dev/null \\\n",
    "    | head -n 1000 \\\n",
    "    | python /home/lngo/intro-to-hadoop/avgRatingMapper03.py \\\n",
    "    | grep Matrix \\\n",
    "    | sort \\\n",
    "    | python /home/lngo/intro-to-hadoop/avgRatingReducer01.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Manual calculation check via python\n",
    "(3.5+4.0+2.5+4.5+2.0)/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Full execution on HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input intro-to-hadoop/movielens/ratings.csv \\\n",
    "    -output intro-to-hadoop/output-movielens-01 \\\n",
    "    -file /home/lngo/intro-to-hadoop/avgRatingMapper03.py \\\n",
    "    -mapper avgRatingMapper03.py \\\n",
    "    -file /home/lngo/intro-to-hadoop/avgRatingReducer01.py \\\n",
    "    -reducer avgRatingReducer01.py \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 First Error!!!\n",
    "\n",
    "Go back to the first few lines of the previously and look for the INFO line **Submitted application application_xxxx_xxxx**. Running the logs command of yarn with the provided application ID is a straightforward way to access all available log information for that application. The syntax to view yarn log is:\n",
    "\n",
    "```\n",
    "!ssh dsciu001 yarn logs -applicationId APPLICATION_ID\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run the yarn view log command here\n",
    "!ssh dsciu001 yarn logs -applicationId application_1476193845089_0123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this information is often massive, as it contains the aggregated logs from all tasks (map and reduce) of the job, which can be in the hundreds. The example below demonstrates this problem by displaying all the possible information of a single-task MapReduce job.\n",
    "In this example, the log of a container has three types of log (LogType): \n",
    "- stderr: Error messages from the actual task execution\n",
    "- stdout: Print out messages if the task includes them\n",
    "- syslog: Logging messages from the Hadoop MapReduce operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "One approach to reduce the number of possible output is to comment out all non-essential lines (lines containing **INFO**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!'yarn logs -applicationId application_1476193845089_0123' | grep -v INFO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Can we refine the information further:\n",
    "- In a MapReduce setting, containers (often) execute the same task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!'yarn logs -applicationId APPLICATION_ID' | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!'yarn logs -applicationId application_1476193845089_0123' | grep '^Container:'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the previous report, we can further identify container information:\n",
    "\n",
    "```\n",
    "Container: container_XXXXXX on  YYYY.palmetto.clemson.edu_ZZZZZ\n",
    "```\n",
    "\n",
    "- Container ID: container_XXXXXX\n",
    "- Address of node where container is placed: YYYY.palmetto.clemson.edu\n",
    "\n",
    "To request yarn to provide a more detailed log at container level, we run:\n",
    "```\n",
    "!\\\n",
    "    'yarn logs -applicationId APPLICATION_ID -containerId CONTAINER_ID --nodeAddress NODE_ADDRESS' \\\n",
    "    | grep -v INFO\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!\\\n",
    "    'yarn logs -applicationId application_1476193845089_0123 -containerId container_e176_1476193845089_0123_01_000028 --nodeAddress dsci001.palmetto.clemson.edu' \\\n",
    "    | grep -v INFO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This error message gives us some insights into the mechanism of Hadoop MapReduce. \n",
    "- Where are the map and reduce python scripts located?\n",
    "- Where would the *movies.csv* file be, if the *-file* flag is used to upload this file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input intro-to-hadoop/movielens/ratings.csv \\\n",
    "    -output intro-to-hadoop/output-movielens-01 \\\n",
    "    -file /home/lngo/intro-to-hadoop/avgRatingMapper04.py \\\n",
    "    -mapper avgRatingMapper04.py \\\n",
    "    -file /home/lngo/intro-to-hadoop/avgRatingReducer01.py \\\n",
    "    -reducer avgRatingReducer01.py \\\n",
    "    -file /home/lngo/intro-to-hadoop/movielens/movies.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Second Error!!!\n",
    "\n",
    "- HDFS is read only. Therefore, all output directories must not have existed prior to job submission\n",
    "- This can be resolved either by specifying a new output directory or deleting the existing output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input intro-to-hadoop/movielens/ratings.csv \\\n",
    "    -output intro-to-hadoop/output-movielens-02 \\\n",
    "    -file /home/lngo/intro-to-hadoop/avgRatingMapper04.py \\\n",
    "    -mapper avgRatingMapper04.py \\\n",
    "    -file /home/lngo/intro-to-hadoop/avgRatingReducer01.py \\\n",
    "    -reducer avgRatingReducer01.py \\\n",
    "    -file /home/lngo/intro-to-hadoop/movielens/movies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -ls intro-to-hadoop/output-movielens-02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -cat intro-to-hadoop/output-movielens-02/part-00000 \\\n",
    "    2>/dev/null | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What about the movies' genres?*\n",
    "- What is being passed from Map to Reduce?\n",
    "- Can reducer do the same thing as mapper, that is, to load in external data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input intro-to-hadoop/movielens/ratings.csv \\\n",
    "    -output intro-to-hadoop/output-movielens-03 \\\n",
    "    -file /home/lngo/intro-to-hadoop/avgRatingMapper02.py \\\n",
    "    -mapper avgRatingMapper02.py \\\n",
    "    -file /home/lngo/intro-to-hadoop/avgRatingReducer02.py \\\n",
    "    -reducer avgRatingReducer02.py \\\n",
    "    -file /home/lngo/intro-to-hadoop/movielens/movies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -ls intro-to-hadoop/output-movielens-03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -cat intro-to-hadoop/output-movielens-03/part-00000 \\\n",
    "    2>/dev/null | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does the number shuffly bytes in this example compare to the previous example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge:\n",
    "\n",
    "1. Modify *avgRatingReducer02.py* so that only movies with averaged ratings higher than 3.75 are collected\n",
    "2. Further enhance your modification so that not only movies with averaged ratings higher than 3.75 are collected but these movies also need to be rated at least 5000 times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Find genres which have the highest average ratings over the years\n",
    "\n",
    "- Identify the genres associated with a movie and its rating: Where to load *movies.csv* - Map side or Reduce side?\n",
    "- Each movie can have multiple genres. This increases the amount of Key/Value pairs being shuffled. What can we do to optimize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input intro-to-hadoop/movielens/ratings.csv \\\n",
    "    -output intro-to-hadoop/output-movielens-04 \\\n",
    "    -file /home/lngo/intro-to-hadoop/avgGenreMapper01.py \\\n",
    "    -mapper avgGenreMapper01.py \\\n",
    "    -file /home/lngo/intro-to-hadoop/avgRatingReducer01.py \\\n",
    "    -reducer avgRatingReducer01.py \\\n",
    "    -file /home/lngo/intro-to-hadoop/movielens/movies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -ls intro-to-hadoop/output-movielens-04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -cat intro-to-hadoop/output-movielens-04/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principle of Big Data Computation: Reduce data movement**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Optimization through in-mapper reduction of Key/Value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -cat intro-to-hadoop/movielens/ratings.csv 2>/dev/null \\\n",
    "    | head -n 10 \\\n",
    "    | python /home/lngo/intro-to-hadoop/avgGenreMapper02.py \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -cat intro-to-hadoop/movielens/ratings.csv 2>/dev/null \\\n",
    "    | head -n 10 \\\n",
    "    | python /home/lngo/intro-to-hadoop/avgGenreMapper02.py \\\n",
    "    | sort \\\n",
    "    | python /home/lngo/intro-to-hadoop/avgGenreReducer01.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make sure that the path to movies.csv is correct inside avgGenreMapper02.py\n",
    "!ssh dsciu001 yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input intro-to-hadoop/movielens/ratings.csv \\\n",
    "    -output intro-to-hadoop/output-movielens-05 \\\n",
    "    -file /home/lngo/intro-to-hadoop/avgGenreMapper02.py \\\n",
    "    -mapper avgGenreMapper02.py \\\n",
    "    -file /home/lngo/intro-to-hadoop/avgGenreReducer01.py \\\n",
    "    -reducer avgGenreReducer01.py \\\n",
    "    -file /home/lngo/intro-to-hadoop/movielens/movies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -cat intro-to-hadoop/output-movielens-05/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How different are the number of shuffle bytes between the two jobs?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Optimization through combiner function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input intro-to-hadoop/text/gutenberg-shakespeare.txt \\\n",
    "    -output intro-to-hadoop/output-wordcount-01 \\\n",
    "    -file /home/lngo/intro-to-hadoop/wordcountMapper.py \\\n",
    "    -mapper wordcountMapper.py \\\n",
    "    -file /home/lngo/intro-to-hadoop/wordcountReducer.py \\\n",
    "    -reducer wordcountReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input intro-to-hadoop/text/gutenberg-shakespeare.txt \\\n",
    "    -output intro-to-hadoop/output-wordcount-02 \\\n",
    "    -file /home/lngo/intro-to-hadoop/wordcountMapper.py \\\n",
    "    -mapper wordcountMapper.py \\\n",
    "    -file /home/lngo/intro-to-hadoop/wordcountReducer.py \\\n",
    "    -reducer wordcountReducer.py \\\n",
    "    -combiner wordcountReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make sure that the path to movies.csv is correct inside avgGenreMapper02.py\n",
    "!ssh dsciu001 yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input intro-to-hadoop/movielens/ratings.csv \\\n",
    "    -output intro-to-hadoop/output-movielens-06 \\\n",
    "    -file /home/lngo/intro-to-hadoop/avgGenreMapper02.py \\\n",
    "    -mapper avgGenreMapper02.py \\\n",
    "    -file /home/lngo/intro-to-hadoop/avgGenreReducer01.py \\\n",
    "    -reducer avgGenreReducer01.py \\\n",
    "    -file /home/lngo/intro-to-hadoop/avgGenreCombiner.py \\\n",
    "    -combiner avgGenreCombiner.py \\\n",
    "    -file /home/lngo/intro-to-hadoop/movielens/movies.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How different are the number of shuffle bytes between the two jobs?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Find users who rate movies most frequently in order to contact them for in-depth marketing analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- How do you define \"frequently\"?\n",
    "    - At least once per week?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input intro-to-hadoop/movielens/ratings.csv \\\n",
    "    -output intro-to-hadoop/output-movielens-07 \\\n",
    "    -file /home/lngo/intro-to-hadoop/userMapper01.py \\\n",
    "    -mapper userMapper01.py \\\n",
    "    -file /home/lngo/intro-to-hadoop/userReducer01.py \\\n",
    "    -reducer userReducer01.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yay, error!!!\n",
    "\n",
    "- You can see Yarn attemps to retry containers\n",
    "- Since these are logical errors and not physical errors, resistance is futile!\n",
    "- Yarn finally gets bored and shuts the job down ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge\n",
    "\n",
    "Get the list of containers from the failed application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!'yarn logs -applicationId application_1476193845089_0282' | grep '^Container:'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Where is the error message?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!\\\n",
    "    'yarn logs -applicationId application_1476193845089_0282 -containerId container_e176_1476193845089_0282_01_000010 --nodeAddress dsci014.palmetto.clemson.edu' \\\n",
    "    | grep -v INFO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that you know what the error is, how can you fix it?**\n",
    "- What is the cause?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -rm -r intro-to-hadoop/output-movielens-07-debug\n",
    "!ssh dsciu001 yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n",
    "    -input intro-to-hadoop/movielens/ratings.csv \\\n",
    "    -output intro-to-hadoop/output-movielens-07-debug \\\n",
    "    -file /home/lngo/intro-to-hadoop/userMapper01.py \\\n",
    "    -mapper userMapper01.py \\\n",
    "    -file /home/lngo/intro-to-hadoop/userDebugReducer01.py \\\n",
    "    -reducer userDebugReducer01.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -ls intro-to-hadoop/output-movielens-07-debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -cat intro-to-hadoop/output-movielens-07-debug/part-00000 2>/dev/null \\\n",
    "    | head -n 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -cat intro-to-hadoop/output-movielens-07-debug/part-00000 2>/dev/null \\\n",
    "    | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge\n",
    "\n",
    "Modify the reducer to correct the above error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge\n",
    "\n",
    "From 2.2, we know which genre has the highest rating. Can we enhance the user study so that only users who provide **frequent** reviews on movies contain the genre with the highest rating are selected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <center> Final Cleanup </center>\n",
    "\n",
    "Executing the cell below will clean up all HDFS output directories created as a result of previous MapReduce programs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -ls intro-to-hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ssh dsciu001 dfs -rm -r intro-to-hadoop/output-wordcount\n",
    "!ssh dsciu001 dfs -rm -r intro-to-hadoop/output-wordcount-01\n",
    "!ssh dsciu001 dfs -rm -r intro-to-hadoop/output-wordcount-02\n",
    "!ssh dsciu001 dfs -rm -r intro-to-hadoop/output-movielens-01\n",
    "!ssh dsciu001 dfs -rm -r intro-to-hadoop/output-movielens-02\n",
    "!ssh dsciu001 dfs -rm -r intro-to-hadoop/output-movielens-03\n",
    "!ssh dsciu001 dfs -rm -r intro-to-hadoop/output-movielens-04\n",
    "!ssh dsciu001 dfs -rm -r intro-to-hadoop/output-movielens-05\n",
    "!ssh dsciu001 dfs -rm -r intro-to-hadoop/output-movielens-06\n",
    "!ssh dsciu001 dfs -rm -r intro-to-hadoop/output-movielens-07\n",
    "!ssh dsciu001 dfs -rm -r intro-to-hadoop/output-movielens-07-debug"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
