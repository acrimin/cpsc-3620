{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Introduction to Spark In-memmory Computing via Python PySpark </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting Started\n",
    "\n",
    "Spark stores data in memory. This memory space is represented by variable **sc** (SparkContext). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7fbe28908050>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark cluster created through this notebook's kernel is running on Cypress' Hadoop infrastructure. Therefore, the easiest storage location for Spark is Cypress' HDFS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir intro-to-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -put /home/lngo/intro-to-spark/airlines/ intro-to-spark/\n",
    "!hdfs dfs -put /home/lngo/intro-to-spark/movielens/ intro-to-spark/\n",
    "!hdfs dfs -put /home/lngo/intro-to-spark/text/ intro-to-spark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls intro-to-hadoop/text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFile = sc.textFile(\"intro-to-spark/text/gutenberg-shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intro-to-spark/text/gutenberg-shakespeare.txt MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:-2\n"
     ]
    }
   ],
   "source": [
    "print (textFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What does Spark do with my data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Storage Level:**\n",
    "- Does RDD use disk?\n",
    "- Does RDD use memory?\n",
    "- Does RDD use off-heap memory?\n",
    "- Should an RDD be serialized (while persisting)?\n",
    "- How many replicas (default: 1) to use (can only be less than 40)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, False, False, False, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intro-to-spark/text/gutenberg-shakespeare.txt MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:-2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, True, False, False, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.getStorageLevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By default, each transformed RDD may be recomputed each time you run an action on it.\n",
    "- It is also possible to *persist* RDD in memory using *persist()* or *cache()*\n",
    "    - *persist()* allows you to specify level of storage for RDD\n",
    "    - *cache()* only persists RDD in memory\n",
    "    - To retire RDD from memory, *unpersist()* is called"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. WordCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data operations in Spark are categorized into two groups, *transformation* and *action*. \n",
    "- A *transformation* creates new dataset from existing data. Examples of *transformation* include map, filter, reduceByKey, and sort. \n",
    "- An *action* returns a value to the driver program (aka memory space of this notebook) after running a computation on the data set. Examples of *action* include count, collect, reduce, and save. \n",
    "\n",
    "\"All transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program.\" -- Spark Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD Operations in Spark\n",
    "\n",
    "**Transformations: **\n",
    "\n",
    "- *map*(f: T -> U) : RDD[T] -> RDD[U]\n",
    "- *filter*(f: T -> Bool) : RDD[T] -> RDD[T]\n",
    "- *flatMap*(f: T -> Seq[U]) : RDD[T] -> RDD[U]\n",
    "- *sample*(*fraction*: Float) : RDD[T] -> RDD[T] (deterministic sampling)\n",
    "- *groupByKey*() : RDD[(K,V)] -> RDD[(K, Seq[V])]\n",
    "- *reduceByKey*(f: (V,V) -> V) : RDD[(K,V)] -> RDD[(K,V)]\n",
    "- *union*() : (RDD[T], RDD[T]) -> RDD[T]\n",
    "- *join*() : (RDD[(K,V)], RDD[(K,W)]) -> RDD[(K,(V,W))]\n",
    "- *cogroup*() : (RDD[(K,V)], RDD[(K,W)] -> RDD[(K, (Seq[V],Seq[W]))]\n",
    "- *crossProduct*() : (RDD[T], RDD[U]) -> RDD[(T,U)]\n",
    "- *mapValues*(f: V -> W) : RDD[(K,V)] -> RDD[(K,W)] (preserves partitioning)\n",
    "- *sort*(c: Comparator[K]) :  RDD[(K,V)] -> RDD[(K,V)]\n",
    "- *partitionBy*(p: Partitioner[K]) : RDD[(K,V)] -> RDD[(K,V)]\n",
    "\n",
    "**Actions:**\n",
    "\n",
    "- *count*() : RDD[T] -> Long\n",
    "- *collect*() : RDD[T] -> Seq[T]\n",
    "- *reduce*(f: (T,T) -> T) : RDD[T] -> T\n",
    "- *lookup*(k : K) : RDD[(K,V)] -> Seq[V] (on hash/range partitionied RDDs)\n",
    "- *save*(path: String) : Outputs RDD to a storage system "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFile = sc.textFile(\"intro-to-spark/text/gutenberg-shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordcount = textFile.flatMap(lambda line: line.split(\" \")) \\\n",
    "            .map(lambda word: (word, 1)) \\\n",
    "            .reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r intro-to-spark/output-wordcount-01\n",
    "wordcount.saveAsTextFile(\"intro-to-spark/output-wordcount-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls intro-to-spark/output-wordcount-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat intro-to-spark/output-wordcount-01/part-00000 \\\n",
    "    2>/dev/null | head -n 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Step-by-step actions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat intro-to-spark/text/gutenberg-shakespeare.txt \\\n",
    "    2>/dev/null | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordcount_step_01 = textFile.flatMap(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordcount_step_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordcount_step_01.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordcount_step_02 = wordcount_step_01.map(lambda word: (word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordcount_step_02.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordcount_step_03 = wordcount_step_02.reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordcount_step_03.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Movie Ratings\n",
    "\n",
    "An independent movie company is looking to invest in a new movie project. With limited finance, the company wants to \n",
    "analyze the reaction of audiences, particularly toward various movie genres, in order to identify beneficial \n",
    "movie project to focus on. The company relies on data collected from a publicly available recommendation service \n",
    "by [MovieLens](http://dl.acm.org/citation.cfm?id=2827872). This \n",
    "[dataset](http://files.grouplens.org/datasets/movielens/ml-10m-README.html) contains **22884377** ratings and **586994**\n",
    " tag applications across **34208** movies. These data were created by **247753** users between January 09, 1995 and January 29, 2016. This dataset was generated on January 29, 2016. \n",
    "\n",
    "From this dataset, several analyses are possible, include the followings:\n",
    "1.   Find movies which have the highest average ratings over the years and identify the corresponding genre.\n",
    "2.   Find genres which have the highest average ratings over the years.\n",
    "3.   Find users who rate movies most frequently in order to contact them for in-depth marketing analysis.\n",
    "\n",
    "These types of analyses, which are somewhat ambiguous, demand the ability to quickly process large amount of data in \n",
    "elatively short amount of time for decision support purposes. In these situations, the sizes of the data typically \n",
    "make analysis done on a single machine impossible and analysis done using a remote storage system impractical. For \n",
    "remainder of the lessons, we will learn how HDFS provides the basis to store massive amount of data and to enable \n",
    "the programming approach to analyze these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls -h intro-to-spark/movielens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat intro-to-hadoop/movielens/README.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat intro-to-hadoop/movielens/links.csv \\\n",
    "    2>/dev/null | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat intro-to-hadoop/movielens/movies.csv \\\n",
    "    2>/dev/null | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat intro-to-hadoop/movielens/ratings.csv \\\n",
    "    2>/dev/null | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cat intro-to-hadoop/movielens/tags.csv \\\n",
    "    2>/dev/null | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings = sc.textFile(\"intro-to-spark/movielens/ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intro-to-spark/movielens/ratings.csv MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:-2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.1 ms, sys: 12.3 ms, total: 38.3 ms\n",
      "Wall time: 55.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22884378"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ratings.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.4 ms, sys: 7.16 ms, total: 26.5 ms\n",
      "Wall time: 41.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22884378"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ratings.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.5 ms, sys: 5.16 ms, total: 20.6 ms\n",
      "Wall time: 32.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22884378"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ratings.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Find movies which have the highest average ratings over the years and identify the corresponding genre\n",
    "\n",
    "- Find the average ratings of all movies over the years\n",
    "- Identify the corresponding genres for each movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'userId,movieId,rating,timestamp',\n",
       " u'1,169,2.5,1204927694',\n",
       " u'1,2471,3.0,1204927438',\n",
       " u'1,48516,5.0,1204927435',\n",
       " u'2,2571,3.5,1436165433']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId,movieId,rating,timestamp\n"
     ]
    }
   ],
   "source": [
    "ratingHeader = ratings.first() #extract header\n",
    "print(ratingHeader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratingsOnly = ratings.filter(lambda x:x != ratingHeader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'1,169,2.5,1204927694',\n",
       " u'1,2471,3.0,1204927438',\n",
       " u'1,48516,5.0,1204927435',\n",
       " u'2,2571,3.5,1436165433',\n",
       " u'2,109487,4.0,1436165496']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingsOnly.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movieRatings = ratingsOnly.map(lambda line: (line.split(\",\")[1], float(line.split(\",\")[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'169', 2.5),\n",
       " (u'2471', 3.0),\n",
       " (u'48516', 5.0),\n",
       " (u'2571', 3.5),\n",
       " (u'109487', 4.0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieRatings.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Possible approaches in aggregating data:** \n",
    "- groupByKey and mapValues\n",
    "- reduceByKey and countByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**groupByKey and mapValues**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'73399', <pyspark.resultiterable.ResultIterable at 0x7f43e43e3490>),\n",
       " (u'110555', <pyspark.resultiterable.ResultIterable at 0x7f43e43ec9d0>),\n",
       " (u'73462', <pyspark.resultiterable.ResultIterable at 0x7f43e43ec950>),\n",
       " (u'145208', <pyspark.resultiterable.ResultIterable at 0x7f43e43ec110>),\n",
       " (u'89373', <pyspark.resultiterable.ResultIterable at 0x7f43e43ec810>)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupByKeyRatings = movieRatings.groupByKey()\n",
    "\n",
    "groupByKeyRatings.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'73399', [3.5, 3.5, 2.5, 3.5, 2.0, 3.5, 2.5, 3.0]),\n",
       " (u'110555', [4.0]),\n",
       " (u'73462',\n",
       "  [1.5,\n",
       "   0.5,\n",
       "   3.0,\n",
       "   1.0,\n",
       "   3.0,\n",
       "   2.5,\n",
       "   5.0,\n",
       "   0.5,\n",
       "   2.0,\n",
       "   2.0,\n",
       "   3.5,\n",
       "   2.5,\n",
       "   5.0,\n",
       "   4.0,\n",
       "   3.0,\n",
       "   1.5,\n",
       "   2.0,\n",
       "   2.5,\n",
       "   0.5,\n",
       "   0.5,\n",
       "   1.5,\n",
       "   0.5,\n",
       "   3.0,\n",
       "   3.0,\n",
       "   4.5,\n",
       "   3.5,\n",
       "   4.0,\n",
       "   0.5,\n",
       "   3.0,\n",
       "   0.5,\n",
       "   0.5,\n",
       "   3.5,\n",
       "   3.0,\n",
       "   1.0,\n",
       "   2.5,\n",
       "   5.0,\n",
       "   1.5,\n",
       "   3.0,\n",
       "   4.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   0.5,\n",
       "   3.0,\n",
       "   0.5,\n",
       "   3.0,\n",
       "   3.0,\n",
       "   0.5,\n",
       "   3.5,\n",
       "   2.5,\n",
       "   1.5,\n",
       "   0.5,\n",
       "   2.0,\n",
       "   0.5,\n",
       "   3.0]),\n",
       " (u'145208', [2.0]),\n",
       " (u'89373', [2.0, 4.0, 3.5, 4.5, 1.5, 2.0, 4.5, 3.0, 3.0, 1.0])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapValuesToListRatings = groupByKeyRatings.mapValues(list)\n",
    "\n",
    "mapValuesToListRatings.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'73399', 3.0),\n",
       " (u'110555', 4.0),\n",
       " (u'73462', 2.2222222222222223),\n",
       " (u'145208', 2.0),\n",
       " (u'89373', 2.9)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgRatings01 = mapValuesToListRatings.mapValues(lambda V: sum(V) / float(len(V)))\n",
    "\n",
    "avgRatings01.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(3.5 + 3.5 + 2.5 + 3.5 + 2.0 + 3.5 + 2.5 + 3.0) / 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**reduceByKey and countByKey**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "countsByKey = movieRatings.countByKey()\n",
    "\n",
    "countsByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'73399', 24.0),\n",
       " (u'110555', 4.0),\n",
       " (u'73462', 120.0),\n",
       " (u'145208', 2.0),\n",
       " (u'89373', 29.0)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sumValues(x,y):\n",
    "    return (x + y)\n",
    "\n",
    "sumRatings = movieRatings.reduceByKey(sumValues)\n",
    "\n",
    "sumRatings.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "sumRatings = movieRatings.reduceByKey(operator.add)\n",
    "sumRatings.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'73399', 3.0),\n",
       " (u'110555', 4.0),\n",
       " (u'73462', 2.2222222222222223),\n",
       " (u'145208', 2.0),\n",
       " (u'89373', 2.9)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgRatings02 = sumRatings.map(lambda x: (x[0], x[1] / countsByKey.get(x[0])))\n",
    "\n",
    "avgRatings02.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we augment movie ratings data with title informations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movies = sc.textFile(\"intro-to-spark/movielens/movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movieId,title,genres\n"
     ]
    }
   ],
   "source": [
    "movieHeader = movies.first() #extract header\n",
    "print(movieHeader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy',\n",
       " u'2,Jumanji (1995),Adventure|Children|Fantasy',\n",
       " u'3,Grumpier Old Men (1995),Comedy|Romance',\n",
       " u'4,Waiting to Exhale (1995),Comedy|Drama|Romance',\n",
       " u'5,Father of the Bride Part II (1995),Comedy']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = movies.filter(lambda x:x != movieHeader)\n",
    "\n",
    "movies.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'1', (u'Toy Story (1995)', u'Adventure|Animation|Children|Comedy|Fantasy')),\n",
       " (u'2', (u'Jumanji (1995)', u'Adventure|Children|Fantasy')),\n",
       " (u'3', (u'Grumpier Old Men (1995)', u'Comedy|Romance')),\n",
       " (u'4', (u'Waiting to Exhale (1995)', u'Comedy|Drama|Romance')),\n",
       " (u'5', (u'Father of the Bride Part II (1995)', u'Comedy'))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieInfo = movies.map(lambda line: (line.split(\",\")[0], (line.split(\",\")[1], line.split(\",\")[2])))\n",
    "\n",
    "movieInfo.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'74701', (3.09375, (u'Of Time and the City (2008)', u'Documentary'))),\n",
       " (u'110557', (3.2, (u'Graceland (2012)', u'Crime|Drama|Thriller'))),\n",
       " (u'121634', (3.0, (u'Open Heart (2013)', u'Documentary'))),\n",
       " (u'110001',\n",
       "  (3.125,\n",
       "   (u'Cosmic Psychos: Blokes You Can Trust (2013)', u'Documentary|Musical'))),\n",
       " (u'3480', (3.4320342205323193, (u'Lucas (1986)', u'Drama|Romance')))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmentedRatings = avgRatings01.join(movieInfo)\n",
    "\n",
    "augmentedRatings.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Movie with highest average rating:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'134613', (5.0, (u'Summer (1976)', u'Comedy|Drama'))),\n",
       " (u'134346',\n",
       "  (5.0, (u'Samay: When Time Strikes (2003)', u'(no genres listed)'))),\n",
       " (u'149456', (5.0, (u'Rainbow Eyes (2007)', u'Romance|Thriller'))),\n",
       " (u'147454', (5.0, (u\"Aleksandr's Price (2013)\", u'Drama|Mystery|Thriller'))),\n",
       " (u'116106',\n",
       "  (5.0, (u'Death of a Nation - The Timor Conspiracy (1994)', u'Documentary'))),\n",
       " (u'139988', (5.0, (u'\"Rimini', u' Rimini (1987)\"'))),\n",
       " (u'141434', (5.0, (u'My Friend Victoria (2014)', u'Drama'))),\n",
       " (u'150766',\n",
       "  (5.0,\n",
       "   (u'One Track Heart: The Story of Krishna Das (2013)', u'Documentary'))),\n",
       " (u'141203', (5.0, (u'Aakrosh (1980)', u'Drama'))),\n",
       " (u'141064', (5.0, (u'Uomo e galantuomo (1975)', u'Comedy')))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmentedRatings.takeOrdered(10, key = lambda x : -x[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Movie with lowest average rating:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'87962', (0.5, (u'Dreamkiller (2010)', u'Thriller'))),\n",
       " (u'139555', (0.5, (u'Kiss the Abyss (2010)', u'Horror'))),\n",
       " (u'94337', (0.5, (u'\"First Texan', u' The (1956)\"'))),\n",
       " (u'140323', (0.5, (u'Memories (2013)', u'Crime|Thriller'))),\n",
       " (u'100203', (0.5, (u'Sundome (2007)', u'Comedy|Drama|Romance'))),\n",
       " (u'139233', (0.5, (u'Sons of Liberty (2013)', u'Action|Sci-Fi'))),\n",
       " (u'110800', (0.5, (u'\"Second Man', u' The (O Defteros Andras) (2013)\"'))),\n",
       " (u'111040',\n",
       "  (0.5,\n",
       "   (u'\"3 Holiday Tails (Golden Christmas 2: The Second Tail',\n",
       "    u' A) (2011)\"'))),\n",
       " (u'90114', (0.5, (u'I Dream Too Much (1935)', u'Comedy|Musical|Romance'))),\n",
       " (u'146858',\n",
       "  (0.5, (u'Il figlio pi\\xf9 piccolo (2010)', u'(no genres listed)')))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmentedRatings.takeOrdered(10, key = lambda x : x[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "- Augment the mapping process of WordCount with a function to filter out punctuations and capitalization from the unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge:\n",
    "\n",
    "1. Make appropriate changes so that only movies with averaged ratings higher than 3.75 are collected\n",
    "2. Further enhance your modification so that only movies with averaged ratings higher than 3.75 and number of ratings of at least 1000 times are collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Find genres which have the highest average ratings over the years\n",
    "\n",
    "- Identify the genres associated with a movie and its rating\n",
    "- Each movie can have multiple genres. How to flip the Key/Value pair?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movieRatings.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movieInfo.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "augmentedInfo = movieRatings.join(movieInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "augmentedInfo.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extractGenreRating (t):\n",
    "    final_tuples = []\n",
    "    genreList = t[1][1][1].split(\"|\")\n",
    "    for genre in genreList:\n",
    "        final_tuples.append((genre,t[1][0]))\n",
    "    return final_tuples\n",
    "\n",
    "print(extractGenreRating((u'1', (3.0, (u'Toy Story (1995)', u'Adventure|Animation|Children|Comedy|Fantasy')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genreRatings = augmentedInfo.flatMap(extractGenreRating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genreRatings.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge:\n",
    "\n",
    "Complete the remaining portion of task 2.2: Calculating the average rating of each genre over the years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Find users who rate movies most frequently in order to contact them for in-depth marketing analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How do you define \"frequently\"?\n",
    "    - At least once per week?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "userRatings = ratingsOnly.map(lambda line: (line.split(\",\")[0], float(line.split(\",\")[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratingGroupByUsers = userRatings.groupByKey().mapValues(list)\n",
    "ratingGroupByUsers.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avgRatingFreq = ratingGroupByUsers.mapValues(lambda V: (max(V) - min(V)) / float(len(V)))\n",
    "avgRatingFreq.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = [1346139060.0,\n",
    "   1346139098.0,\n",
    "   1346139113.0,\n",
    "   1346139053.0,\n",
    "   1346139234.0,\n",
    "   1346139006.0,\n",
    "   1346139209.0,\n",
    "   1346139147.0,\n",
    "   1346138998.0,\n",
    "   1346139206.0,\n",
    "   1346139224.0,\n",
    "   1346139174.0,\n",
    "   1346139152.0,\n",
    "   1346139230.0,\n",
    "   1346139181.0,\n",
    "   1346139159.0,\n",
    "   1346139314.0]\n",
    "(max(x) - min(x)) / float(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topUsers = avgRatingFreq.top(10, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topUsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. Airlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spark SQL**\n",
    "- Spark module for structured data processing\n",
    "- provide more information about the structure of both the data and the computation being performed for additional optimization\n",
    "- execute SQL queries written using either a basic SQL syntax or HiveQL\n",
    "\n",
    "**DataFrame**\n",
    "- distributed collection of data organized into named columns\n",
    "- conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood\n",
    "- can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x7fefac382c10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "airlines = sqlContext.read.format(\"com.databricks.spark.csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferschema\", \"true\")\\\n",
    "    .load(\"intro-to-hadoop/airlines/data/\")\\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.2 ms, sys: 8.07 ms, total: 40.3 ms\n",
      "Wall time: 3min 17s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "123534969"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "airlines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.12 ms, sys: 278 µs, total: 1.4 ms\n",
      "Wall time: 489 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "123534969"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "airlines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: string (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- CRSElapsedTime: string (nullable = true)\n",
      " |-- AirTime: string (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: string (nullable = true)\n",
      " |-- TaxiIn: string (nullable = true)\n",
      " |-- TaxiOut: string (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- CarrierDelay: string (nullable = true)\n",
      " |-- WeatherDelay: string (nullable = true)\n",
      " |-- NASDelay: string (nullable = true)\n",
      " |-- SecurityDelay: string (nullable = true)\n",
      " |-- LateAircraftDelay: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airlines.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "You can interact with a DataFrame via SQLContext using SQL statements by registerting the DataFrame as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "airlines.registerTempTable(\"airlines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How many unique airlines are there?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|UniqueCarrier|\n",
      "+-------------+\n",
      "|           AA|\n",
      "|       PA (1)|\n",
      "|           TW|\n",
      "|           TZ|\n",
      "|           AQ|\n",
      "|           HA|\n",
      "|           AS|\n",
      "|           UA|\n",
      "|           B6|\n",
      "|           NW|\n",
      "|           HP|\n",
      "|           US|\n",
      "|           OH|\n",
      "|           OO|\n",
      "|           PI|\n",
      "|           CO|\n",
      "|       ML (1)|\n",
      "|           PS|\n",
      "|           WN|\n",
      "|           DH|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uniqueAirline = sqlContext.sql(\"SELECT DISTINCT UniqueCarrier \\\n",
    "                                FROM airlines\")\n",
    "uniqueAirline.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Calculate how many flights completed by each carrier over time*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n",
      "|UniqueCarrier|FlightCount|\n",
      "+-------------+-----------+\n",
      "|           AA|   14984647|\n",
      "|       PA (1)|     316167|\n",
      "|           TW|    3757747|\n",
      "|           TZ|     208420|\n",
      "|           AQ|     154381|\n",
      "|           HA|     274265|\n",
      "|           AS|    2878021|\n",
      "|           UA|   13299817|\n",
      "|           B6|     811341|\n",
      "|           NW|   10292627|\n",
      "|           HP|    3636682|\n",
      "|           US|   14075530|\n",
      "|           OH|    1464176|\n",
      "|           OO|    3090853|\n",
      "|           PI|     873957|\n",
      "|           CO|    8145788|\n",
      "|       ML (1)|      70622|\n",
      "|           PS|      83617|\n",
      "|           WN|   15976022|\n",
      "|           DH|     693047|\n",
      "+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 2.03 ms, sys: 1.81 ms, total: 3.84 ms\n",
      "Wall time: 2.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "carrierFlightCount = sqlContext.sql(\"SELECT UniqueCarrier, COUNT(UniqueCarrier) AS FlightCount \\\n",
    "                                    FROM airlines GROUP BY UniqueCarrier\")\n",
    "carrierFlightCount.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "*How do you display full carrier names?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "carriers = sqlContext.read.format(\"com.databricks.spark.csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferschema\", \"true\")\\\n",
    "    .load(\"intro-to-hadoop/airlines/metadata/carriers.csv\")\\\n",
    "    .cache()\n",
    "carriers.registerTempTable(\"carriers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Code: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "carriers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-----------+\n",
      "|         Description|UniqueCarrier|FlightCount|\n",
      "+--------------------+-------------+-----------+\n",
      "|Pinnacle Airlines...|           9E|     521059|\n",
      "|American Airlines...|           AA|   14984647|\n",
      "| Aloha Airlines Inc.|           AQ|     154381|\n",
      "|Alaska Airlines Inc.|           AS|    2878021|\n",
      "|     JetBlue Airways|           B6|     811341|\n",
      "|Continental Air L...|           CO|    8145788|\n",
      "|    Independence Air|           DH|     693047|\n",
      "|Delta Air Lines Inc.|           DL|   16547870|\n",
      "|Eastern Air Lines...|           EA|     919785|\n",
      "|Atlantic Southeas...|           EV|    1697172|\n",
      "|Frontier Airlines...|           F9|     336958|\n",
      "|AirTran Airways C...|           FL|    1265138|\n",
      "|Hawaiian Airlines...|           HA|     274265|\n",
      "|America West Airl...|           HP|    3636682|\n",
      "|Midway Airlines I...|       ML (1)|      70622|\n",
      "|American Eagle Ai...|           MQ|    3954895|\n",
      "|Northwest Airline...|           NW|   10292627|\n",
      "|         Comair Inc.|           OH|    1464176|\n",
      "|Skywest Airlines ...|           OO|    3090853|\n",
      "|Pan American Worl...|       PA (1)|     316167|\n",
      "+--------------------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 4.02 ms, sys: 4.05 ms, total: 8.07 ms\n",
      "Wall time: 18.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "carrierFlightCountFullName = sqlContext.sql(\"SELECT c.Description, a.UniqueCarrier, COUNT(a.UniqueCarrier) AS FlightCount \\\n",
    "                                    FROM airlines AS a \\\n",
    "                                    INNER JOIN carriers AS c \\\n",
    "                                    ON c.Code = a.UniqueCarrier \\\n",
    "                                    GROUP BY a.UniqueCarrier, c.Description \\\n",
    "                                    ORDER BY a.UniqueCarrier\")\n",
    "carrierFlightCountFullName.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What is the averaged departure delay time for each airline?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "avgDepartureDelay = sqlContext.sql(\"SELECT c.Description, a.UniqueCarrier, AVG(a.DepDelay) AS AvgDepDelay \\\n",
    "                                    FROM airlines AS a \\\n",
    "                                    INNER JOIN carriers AS c \\\n",
    "                                    ON c.Code = a.UniqueCarrier \\\n",
    "                                    GROUP BY a.DepDelay, c.Description \\\n",
    "                                    ORDER BY a.UniqueCarrier\")\n",
    "carrierFlightCountFullName.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "airlines.unpersist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda 4.2.0 (Python 2) PySpark on Cypress",
   "language": "",
   "name": "remote-dsciu001-python2-pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
