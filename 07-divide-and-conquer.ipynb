{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Partitioning: Divide and Conquer</center>\n",
    "### <center> Linh B. Ngo </center>\n",
    "### <center> CPSC 3620 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <center> Partitioning </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Partitioning simply divides the problem into parts and then compute the parts and combine results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The basis of all parallel programming, in one form or another. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Pleasantly parallel used partitioning without any interaction between the parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Most partitioning  formulation require the results of the parts to be combined to obtain the desired results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Partitioning can be applied to the program data. This is call data partitioning or domain decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Partitioning can also be applied to the functions of a program. This is called functional decomposition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <center> Divide and Conquer </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Characterized by dividing problem into sub-problems of same form as larger problem. Further divisions into still smaller sub-problems, usually done by recursion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Recursive divide and conquer amenable to parallelization because separate processes can be used for divided pairs. Also usually data is naturally localized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/07/dc01.png\" width=\"700\"/> \n",
    "<sub>Wilkinson, Barry, and Michael Allen. Parallel programming. 2nd Ed. 2003. </sub>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/07/dc02.png\" width=\"700\"/> \n",
    "<sub>Wilkinson, Barry, and Michael Allen. Parallel programming. 2nd Ed. 2003. </sub>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/mpi4py/divide.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/mpi4py/divide.py\n",
    "#!/usr/bin/env python\n",
    "# divide.py\n",
    "from mpi4py import MPI\n",
    "import math\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank(); size = comm.Get_size()\n",
    "for i in range(0, int(math.log2(size))):\n",
    "    distance = int(math.pow(2, i))\n",
    "    if ( rank < int(math.pow(2, i)) ):\n",
    "        print (\"Iteration \", i, \": sender \", rank, \n",
    "               \" sends to \", rank + distance)\n",
    "    if (rank >= int(math.pow(2, i)) and rank < int(math.pow(2,i+1))):\n",
    "        print (\"Iteration \", i, \": receiver \", rank, \n",
    "               \"receives from \", rank - distance)\n",
    "    #am I sender or receiver?\n",
    "    # who am I sending/receiving to/from?    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0 : receiver  1 receives from  0\r\n",
      "Iteration  1 : sender  1  sends to  3\r\n",
      "Iteration  2 : sender  1  sends to  5\r\n",
      "Iteration  1 : receiver  2 receives from  0\r\n",
      "Iteration  2 : sender  2  sends to  6\r\n",
      "Iteration  1 : receiver  3 receives from  1\r\n",
      "Iteration  2 : sender  3  sends to  7\r\n",
      "Iteration  2 : receiver  4 receives from  0\r\n",
      "Iteration  2 : receiver  5 receives from  1\r\n",
      "Iteration  2 : receiver  6 receives from  2\r\n",
      "Iteration  2 : receiver  7 receives from  3\r\n",
      "Iteration  0 : sender  0  sends to  1\r\n",
      "Iteration  1 : sender  0  sends to  2\r\n",
      "Iteration  2 : sender  0  sends to  4\r\n"
     ]
    }
   ],
   "source": [
    "!chmod 755 codes/mpi4py/divide.py\n",
    "!module load gcc/5.3.0 openmpi/1.10.3; mpirun -np 8 codes/mpi4py/divide.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center> <img src=\"pictures/07/dc03.png\" width=\"700\"/> \n",
    "<sub>Wilkinson, Barry, and Michael Allen. Parallel programming. 2nd Ed. 2003. </sub>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/mpi4py/divide.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/mpi4py/conquer.py\n",
    "#!/usr/bin/env python\n",
    "# conquer.py\n",
    "from mpi4py import MPI\n",
    "import math\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank(); size = comm.Get_size()\n",
    "for i in range(0, int(math.log2(size))):\n",
    "    distance = int(math.pow(2, i))\n",
    "    if ( rank < int(math.pow(2, i)) ):\n",
    "        print (\"Iteration \", i, \": sender \", rank, \n",
    "               \" sends to \", rank + distance)\n",
    "    if (rank >= int(math.pow(2, i)) and rank < int(math.pow(2,i+1))):\n",
    "        print (\"Iteration \", i, \": receiver \", rank, \n",
    "               \"receives from \", rank - distance)\n",
    "    #am I sender or receiver?\n",
    "    # who am I sending/receiving to/from?    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0 : receiver  1 receives from  0\r\n",
      "Iteration  1 : sender  1  sends to  3\r\n",
      "Iteration  2 : sender  1  sends to  5\r\n",
      "Iteration  1 : receiver  2 receives from  0\r\n",
      "Iteration  2 : sender  2  sends to  6\r\n",
      "Iteration  1 : receiver  3 receives from  1\r\n",
      "Iteration  2 : sender  3  sends to  7\r\n",
      "Iteration  2 : receiver  4 receives from  0\r\n",
      "Iteration  2 : receiver  5 receives from  1\r\n",
      "Iteration  2 : receiver  6 receives from  2\r\n",
      "Iteration  2 : receiver  7 receives from  3\r\n",
      "Iteration  0 : sender  0  sends to  1\r\n",
      "Iteration  1 : sender  0  sends to  2\r\n",
      "Iteration  2 : sender  0  sends to  4\r\n"
     ]
    }
   ],
   "source": [
    "!chmod 755 codes/mpi4py/conquer.py\n",
    "!module load gcc/5.3.0 openmpi/1.10.3; mpirun -np 8 codes/mpi4py/conquer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Many sorting algorithms can be parallelized by partitioning using\n",
    "divide and conquer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <center> Bucket Sort </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Sequential sorting time complexity: $O(nlog(n/m))$\n",
    "- Works well if the original numbers uniformly distributed across a known interval, say 0 to a - 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/07/bucketsort1.png\" width=\"700\"/> \n",
    "<sub>Wilkinson, Barry, and Michael Allen. Parallel programming. 2nd Ed. 2003. </sub>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Simple approach to parallel bucket sort **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center> <img src=\"pictures/07/bucketsort2.png\" width=\"700\"/> \n",
    "<sub>Wilkinson, Barry, and Michael Allen. Parallel programming. 2nd Ed. 2003. </sub>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Bcast data\n",
    "- Sort only those elements that fit in local interval bucket (determined by rank)\n",
    "- Gather sorted bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Uppercase collective: MPI.Scatterv**\n",
    "\n",
    "Comm.Scatterv([sendbuf, tuple_int sendcounts, tuple_int displacements, MPI_Datatype sendtype], recvbuf, root=0)\n",
    "\n",
    "Parameters:\t\n",
    "- Comm (MPI comm) – communicator across which to scatter\n",
    "- sendbuf (choice) – buffer\n",
    "- sendcounts (tuple_int) – number of elements to send to each process (one integer for each process)\n",
    "- displacements (tuple_int) – number of elements away from the first element in the array at which to begin the new, segmented array\n",
    "- sendtype (MPI_Datatype) – MPI datatype of the buffer being sent (choice of sendbuf)\n",
    "- recvbuf (choice) – buffer in which to receive the sendbuf\n",
    "- root (int) – process from which to scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/mpi4py/scatterv.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/mpi4py/scatterv.py\n",
    "#!/usr/bin/env python\n",
    "# scatterv.py\n",
    "# Run with 3 processes\n",
    "import numpy\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD; rank = comm.Get_rank()\n",
    "if rank == 0:\n",
    "    x = numpy.linspace(0,100,11)\n",
    "    print (x)\n",
    "else:\n",
    "    x = None\n",
    "if rank == 2:\n",
    "    xlocal = numpy.zeros(9)\n",
    "else:\n",
    "    xlocal = numpy.zeros(1)\n",
    "comm.Scatterv([x,(1,1,9),(0,1,2),MPI.DOUBLE],xlocal)\n",
    "print (\"process \" + str(rank) + \" has \" +str(xlocal))\n",
    "if (rank == 0):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0.   10.   20.   30.   40.   50.   60.   70.   80.   90.  100.]\r\n",
      "process 0 has [ 0.]\r\n",
      "[   0.   10.   20.   30.   40.   50.   60.   70.   80.   90.  100.]\r\n",
      "process 1 has [ 10.]\r\n",
      "process 2 has [  20.   30.   40.   50.   60.   70.   80.   90.  100.]\r\n"
     ]
    }
   ],
   "source": [
    "!chmod 755 codes/mpi4py/scatterv.py\n",
    "!module load gcc/5.3.0 openmpi/1.10.3; mpirun -np 3 codes/mpi4py/scatterv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Uppercase collective: MPI.Gatherv**\n",
    "\n",
    "Comm.Gatherv(sendbuf, [recvbuf, tuple_int sendcounts, tuple_int displacements, MPI_Datatype sendtype], root=0)\n",
    "\n",
    "Parameters:\t\n",
    "- Comm (MPI comm) – communicator across which to scatter\n",
    "- sendbuf (choice) – buffer\n",
    "- recvbuf (choice) – buffer in which to receive the sendbuf\n",
    "- sendcounts (tuple_int) – number of elements to receive from each process (one integer for each process)\n",
    "- displacements (tuple_int) – number of elements away from the first element in the receiving array at which to begin appending the segmented array\n",
    "- sendtype (MPI_Datatype) – MPI datatype of the buffer being sent (choice of sendbuf)\n",
    "- root (int) – process from which to scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/mpi4py/gatherv.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/mpi4py/gatherv.py\n",
    "#!/usr/bin/env python\n",
    "# gatherv.py\n",
    "# Run with 3 processes\n",
    "import numpy\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD; rank = comm.Get_rank()\n",
    "if rank == 0:\n",
    "    x = numpy.linspace(0,100,11)\n",
    "else:\n",
    "    x = None\n",
    "if rank == 2:\n",
    "    xlocal = numpy.zeros(9)\n",
    "else:\n",
    "    xlocal = numpy.zeros(1)\n",
    "comm.Scatterv([x,(1,1,9),(0,1,2),MPI.DOUBLE],xlocal); \n",
    "comm.Barrier()\n",
    "if rank == 0:\n",
    "    xGathered = numpy.zeros(11)\n",
    "else:\n",
    "    xGathered = None\n",
    "comm.Gatherv(xlocal,[xGathered,(1,1,9),(0,1,2),MPI.DOUBLE])\n",
    "print (xlocal); \n",
    "print (\"process \" + str(rank) + \" has \" +str(xGathered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.]\r\n",
      "process 0 has [   0.   10.   20.   30.   40.   50.   60.   70.   80.   90.  100.]\r\n",
      "[ 10.]\r\n",
      "process 1 has None\r\n",
      "[  20.   30.   40.   50.   60.   70.   80.   90.  100.]\r\n",
      "process 2 has None\r\n"
     ]
    }
   ],
   "source": [
    "!chmod 755 codes/mpi4py/gatherv.py\n",
    "!module load gcc/5.3.0 openmpi/1.10.3; mpirun -np 3 codes/mpi4py/gatherv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Parallel Bucket Sort version 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/mpi4py/bucket1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/mpi4py/bucket1.py\n",
    "#!/usr/bin/env python\n",
    "# bucket1.py\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "rank = comm.Get_rank(); size = comm.Get_size(); N = 16\n",
    "\n",
    "unsorted = np.zeros(N, dtype=\"int\")\n",
    "final_sorted = np.zeros(N, dtype=\"int\")\n",
    "\n",
    "if rank == 0:\n",
    "    unsorted = np.random.randint(low=0,high=N,size=N)\n",
    "    \n",
    "comm.Bcast(unsorted)\n",
    "local_min = rank * N / size\n",
    "local_max = (rank + 1) * N / size\n",
    "\n",
    "#generic form:\n",
    "# local_min = rank * (b - a) / size\n",
    "# local_max = (rank + 1) * (b - a) / size\n",
    "\n",
    "print(\"At rank \", rank, \": \", local_min, local_max)\n",
    "\n",
    "local_bucket = unsorted[np.logical_and(unsorted >= local_min, \n",
    "                                       unsorted < local_max)]\n",
    "local_sorted = np.sort(local_bucket)\n",
    "\n",
    "arr_sendcount = np.zeros(size, dtype=\"int\")\n",
    "sendcount = np.array([local_sorted.size], dtype=\"int\")\n",
    "\n",
    "comm.Gather(sendcount, arr_sendcount, root=0)\n",
    "comm.Bcast(arr_sendcount, root = 0)\n",
    "\n",
    "dispcount = np.zeros(size, dtype=\"int\")\n",
    "dispcount[0] = 0\n",
    "for i in range(1,size):\n",
    "    dispcount[i] = dispcount[i-1] + arr_sendcount[i-1]\n",
    "    \n",
    "comm.Gatherv(local_sorted, [final_sorted, \n",
    "                            tuple(arr_sendcount), \n",
    "                            tuple(dispcount), \n",
    "                            MPI.DOUBLE])\n",
    "\n",
    "if (rank == 0):\n",
    "    print (\"At root: \", unsorted)\n",
    "    print (\"At root: \", final_sorted)\n",
    "print(\"Rank\", rank, \": \", local_bucket)\n",
    "print(\"Rank\", rank, \": \", local_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At rank  3 :  6.0 8.0\r\n",
      "Rank 3 :  [7]\r\n",
      "Rank 3 :  [7]\r\n",
      "At rank  4 :  8.0 10.0\r\n",
      "Rank 4 :  [8 8]\r\n",
      "Rank 4 :  [8 8]\r\n",
      "At rank  5 :  10.0 12.0\r\n",
      "Rank 5 :  [10]\r\n",
      "Rank 5 :  [10]\r\n",
      "At rank  6 :  12.0 14.0\r\n",
      "Rank 6 :  [12]\r\n",
      "Rank 6 :  [12]\r\n",
      "At rank  7 :  14.0 16.0\r\n",
      "Rank 7 :  [15]\r\n",
      "Rank 7 :  [15]\r\n",
      "At rank  0 :  0.0 2.0\r\n",
      "At root:  [ 5  7  5  3 10  3  8 12  1  5  2 15  4  8  5  5]\r\n",
      "At root:  [ 1  2  3  3  4  5  5  5  5  5  7  8  8 10 12 15]\r\n",
      "Rank 0 :  [1]\r\n",
      "At rank  1 :  2.0 4.0\r\n",
      "Rank 1 :  [3 3 2]\r\n",
      "Rank 1 :  [2 3 3]\r\n",
      "At rank  2 :  4.0 6.0\r\n",
      "Rank 2 :  [5 5 5 4 5 5]\r\n",
      "Rank 2 :  [4 5 5 5 5 5]\r\n",
      "Rank 0 :  [1]\r\n"
     ]
    }
   ],
   "source": [
    "!chmod 755 codes/mpi4py/bucket1.py\n",
    "!module load gcc/5.3.0 openmpi/1.10.3; mpirun -np 8 codes/mpi4py/bucket1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/07/bucketsort3.png\" width=\"700\"/> \n",
    "<sub>Wilkinson, Barry, and Michael Allen. Parallel programming. 2nd Ed. 2003. </sub>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/07/all2all.png\" width=\"700\"/> \n",
    "<sub>Wilkinson, Barry, and Michael Allen. Parallel programming. 2nd Ed. 2003. </sub>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/07/all2all_2.png\" width=\"700\"/> \n",
    "<sub>Wilkinson, Barry, and Michael Allen. Parallel programming. 2nd Ed. 2003. </sub>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Uppercase collective: MPI.Alltoallv**\n",
    "\n",
    "Comm.Alltoallv([sendbuf, tuple_int sendcounts, tuple_int displacements, MPI_Datatype sendtype],[recvbuf, tuple_int recvcounts, tuple_int displacements, MPI_Datatype sendtype])\n",
    "\n",
    "Parameters:\t\n",
    "- Comm (MPI comm) – communicator across which to scatter\n",
    "- sendbuf (choice) – buffer\n",
    "- recvbuf (choice) – buffer in which to receive the sendbuf\n",
    "- sendcounts (tuple_int) – number of elements to send to each process (one integer for each process)\n",
    "- displacements (tuple_int) – number of elements away from the first element in the sending array at which to begin sending the segmented array\n",
    "- sendtype (MPI_Datatype) – MPI datatype of the buffer being sent (choice of sendbuf)\n",
    "- recvcounts (tuple_int) – number of elements to receive from each process (one integer for each process)\n",
    "- displacements (tuple_int) – number of elements away from the first element in the receiving array at which to begin appending the segmented array\n",
    "- sendtype (MPI_Datatype) – MPI datatype of the buffer being receive (choice of recvbuf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/mpi4py/all2allv.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/mpi4py/all2allv.py\n",
    "#!/usr/bin/env python\n",
    "# all2allv.py\n",
    "import numpy\n",
    "import random\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank(); size = comm.Get_size(); N = 8\n",
    "local_array = numpy.random.randint(low = 0, high = N, size = N)\n",
    "print(\"Rank \", rank, \"local_array: \", local_array)\n",
    "#stackoverflow\n",
    "def constrained_sum_sample_pos(n, total):\n",
    "    dividers = sorted(random.sample(range(1, total), n - 1))\n",
    "    return [a - b for a, b in zip(dividers + [total], [0] + dividers)]\n",
    "sendcount = numpy.array(constrained_sum_sample_pos(size, N), dtype=\"int\")\n",
    "dispcount = numpy.zeros(size, dtype=\"int\")\n",
    "dispcount[0] = 0\n",
    "for i in range(1,size):\n",
    "    dispcount[i] = dispcount[i-1] + sendcount[i-1]    \n",
    "print (\"Rank \", rank, \" sendcount: \", sendcount)\n",
    "print (\"Rank \", rank, \" dispcount: \", dispcount)\n",
    "recvcount = numpy.zeros(size, dtype=\"int\")\n",
    "comm.Alltoall(sendcount, recvcount)\n",
    "print(\"Rank \", rank, \" recvcount: \", recvcount)\n",
    "disprecv = numpy.zeros(size, dtype=\"int\")\n",
    "disprecv[0] = 0\n",
    "for i in range(1,size):\n",
    "    disprecv[i] = disprecv[i-1] + recvcount[i-1]   \n",
    "print(\"Rank \", rank, \" disprecv: \", disprecv)\n",
    "new_array = numpy.zeros(numpy.sum(recvcount), dtype=\"int\")\n",
    "comm.Alltoallv([local_array,tuple(sendcount),tuple(dispcount),MPI.DOUBLE],\n",
    "               [new_array, tuple(recvcount), tuple(disprecv), MPI.DOUBLE])\n",
    "print (\"Rank \", rank, \"new_array: \", new_array)\n",
    "#print (local_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank  2 local_array:  [7 0 4 7 2 6 5 3]\r\n",
      "Rank  2  sendcount:  [2 1 2 3]\r\n",
      "Rank  3 local_array:  [4 2 4 4 0 7 4 4]\r\n",
      "Rank  3  sendcount:  [3 1 3 1]\r\n",
      "Rank  0 local_array:  [4 5 5 3 1 5 3 7]\r\n",
      "Rank  0  sendcount:  [1 1 3 3]\r\n",
      "Rank  1 local_array:  [3 5 3 7 2 5 1 3]\r\n",
      "Rank  1  sendcount:  [2 1 2 3]\r\n",
      "Rank  1  dispcount:  [0 2 3 5]\r\n",
      "Rank  1  recvcount:  [1 1 1 1]\r\n",
      "Rank  3  dispcount:  [0 3 4 7]\r\n",
      "Rank  3  recvcount:  [3 3 3 1]\r\n",
      "Rank  0  dispcount:  [0 1 2 5]\r\n",
      "Rank  0  recvcount:  [1 2 2 3]\r\n",
      "Rank  2  dispcount:  [0 2 3 5]\r\n",
      "Rank  2  recvcount:  [3 2 2 3]\r\n",
      "Rank  2  disprecv:  [0 3 5 7]\r\n",
      "Rank  0  disprecv:  [0 1 3 5]\r\n",
      "Rank  3  disprecv:  [0 3 6 9]\r\n",
      "Rank  1  disprecv:  [0 1 2 3]\r\n",
      "Rank  2 new_array:  [5 3 1 7 2 7 2 0 7 4]\r\n",
      "Rank  1 new_array:  [5 3 4 4]\r\n",
      "Rank  3 new_array:  [5 3 7 5 1 3 6 5 3 4]\r\n",
      "Rank  0 new_array:  [4 3 5 7 0 4 2 4]\r\n"
     ]
    }
   ],
   "source": [
    "!chmod 755 codes/mpi4py/all2allv.py\n",
    "!module load gcc/5.3.0 openmpi/1.10.3; mpirun -np 4 codes/mpi4py/all2allv.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting codes/mpi4py/bucket2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile codes/mpi4py/bucket2.py\n",
    "#!/usr/bin/env python\n",
    "# bucket2.py\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank(); size = comm.Get_size(); N = 16\n",
    "unsorted = None\n",
    "local_unsorted = np.zeros(int(N / size), dtype=\"int\")\n",
    "final_sorted = np.zeros(N, dtype=\"int\")\n",
    "if rank == 0:\n",
    "    unsorted = np.random.randint(low=0,high=N,size=N)\n",
    "    print (\"Original unsorted data at process\", rank, \": \", unsorted)\n",
    "comm.Scatter(unsorted, local_unsorted, root = 0)\n",
    "print (\"Scattered data at process \", rank, \": \", local_unsorted)\n",
    "\n",
    "local_buckets = np.full(shape=(int(N/size), size), fill_value=-1, dtype=\"int\")\n",
    "sendcount = np.zeros(size, dtype=\"int\")\n",
    "local_unsorted_bucketed = np.zeros(int(N / size), dtype=\"int\")\n",
    "count = 0\n",
    "for i in range(0, size):    \n",
    "    for j in range (0, local_unsorted.size):\n",
    "        local_min = i * N/size\n",
    "        local_max = (i + 1) * N / size  \n",
    "        if ((local_unsorted[j] >= local_min) and (local_unsorted[j] < local_max)):\n",
    "            local_buckets[i][sendcount[i]] = local_unsorted[j]\n",
    "            sendcount[i] += 1\n",
    "    if (sendcount[i] > 0):\n",
    "        local_unsorted_bucketed[count:count + sendcount[i]] = local_buckets[i, 0:sendcount[i]]\n",
    "        print (\"Local_unsorted_bucketed at process \", rank, \": \", local_unsorted_bucketed)\n",
    "        count += sendcount[i]\n",
    "\n",
    "print (\"Sendcount at process \", rank, \": \", sendcount)\n",
    "print (\"Bucket matrix at process \", rank, \": \\n\", local_buckets)\n",
    "\n",
    "dispcount = np.zeros(size, dtype=\"int\")\n",
    "dispcount[0] = 0\n",
    "for i in range(1,size):\n",
    "    dispcount[i] = dispcount[i-1] + sendcount[i-1]    \n",
    "\n",
    "recvcount = np.zeros(size, dtype=\"int\")\n",
    "comm.Alltoall(sendcount, recvcount)\n",
    "print(\"Rank \", rank, \" recvcount: \", recvcount)\n",
    "disprecv = np.zeros(size, dtype=\"int\")\n",
    "disprecv[0] = 0\n",
    "for i in range(1,size):\n",
    "    disprecv[i] = disprecv[i-1] + recvcount[i-1]   \n",
    "print(\"Rank \", rank, \" disprecv: \", disprecv)\n",
    "single_unsorted_bucket = np.zeros(np.sum(recvcount), dtype=\"int\")\n",
    "comm.Alltoallv([local_unsorted_bucketed,tuple(sendcount),tuple(dispcount),MPI.DOUBLE],\n",
    "               [single_unsorted_bucket, tuple(recvcount), tuple(disprecv), MPI.DOUBLE])\n",
    "print (\"Rank \", rank, \" single_unsorted_bucket: \", single_unsorted_bucket)\n",
    "local_sorted = np.sort(single_unsorted_bucket)\n",
    "arr_sendcount = np.zeros(size, dtype=\"int\")\n",
    "sendcount = np.array([local_sorted.size], dtype=\"int\")\n",
    "comm.Gather(sendcount, arr_sendcount, root=0)\n",
    "comm.Bcast(arr_sendcount, root = 0)\n",
    "dispcount = np.zeros(size, dtype=\"int\")\n",
    "dispcount[0] = 0\n",
    "for i in range(1,size):\n",
    "    dispcount[i] = dispcount[i-1] + arr_sendcount[i-1]\n",
    "comm.Gatherv(local_sorted, [final_sorted, tuple(arr_sendcount), tuple(dispcount), MPI.DOUBLE])\n",
    "if (rank == 0):\n",
    "    print (final_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scattered data at process  2 :  [ 5  1 11  6]\r\n",
      "Local_unsorted_bucketed at process  2 :  [1 0 0 0]\r\n",
      "Scattered data at process  3 :  [ 2  8 15  8]\r\n",
      "Local_unsorted_bucketed at process  3 :  [2 0 0 0]\r\n",
      "Original unsorted data at process 0 :  [ 9 14 11 13  0  2 12  1  5  1 11  6  2  8 15  8]\r\n",
      "Scattered data at process  0 :  [ 9 14 11 13]\r\n",
      "Local_unsorted_bucketed at process  0 :  [ 9 11  0  0]\r\n",
      "Local_unsorted_bucketed at process  0 :  [ 9 11 14 13]\r\n",
      "Scattered data at process  1 :  [ 0  2 12  1]\r\n",
      "Local_unsorted_bucketed at process  1 :  [0 2 1 0]\r\n",
      "Local_unsorted_bucketed at process  2 :  [1 5 6 0]\r\n",
      "Local_unsorted_bucketed at process  2 :  [ 1  5  6 11]\r\n",
      "Sendcount at process  2 :  [1 2 1 0]\r\n",
      "Bucket matrix at process  2 : \r\n",
      " [[ 1 -1 -1 -1]\r\n",
      " [ 5  6 -1 -1]\r\n",
      " [11 -1 -1 -1]\r\n",
      " [-1 -1 -1 -1]]\r\n",
      "Rank  2  recvcount:  [2 0 1 2]\r\n",
      "Local_unsorted_bucketed at process  3 :  [2 8 8 0]\r\n",
      "Local_unsorted_bucketed at process  3 :  [ 2  8  8 15]\r\n",
      "Sendcount at process  3 :  [1 0 2 1]\r\n",
      "Bucket matrix at process  3 : \r\n",
      " [[ 2 -1 -1 -1]\r\n",
      " [-1 -1 -1 -1]\r\n",
      " [ 8  8 -1 -1]\r\n",
      " [15 -1 -1 -1]]\r\n",
      "Rank  3  recvcount:  [2 1 0 1]\r\n",
      "Sendcount at process  0 :  [0 0 2 2]\r\n",
      "Bucket matrix at process  0 : \r\n",
      " [[-1 -1 -1 -1]\r\n",
      " [-1 -1 -1 -1]\r\n",
      " [ 9 11 -1 -1]\r\n",
      " [14 13 -1 -1]]\r\n",
      "Rank  0  recvcount:  [0 3 1 1]\r\n",
      "Local_unsorted_bucketed at process  1 :  [ 0  2  1 12]\r\n",
      "Sendcount at process  1 :  [3 0 0 1]\r\n",
      "Bucket matrix at process  1 : \r\n",
      " [[ 0  2  1 -1]\r\n",
      " [-1 -1 -1 -1]\r\n",
      " [-1 -1 -1 -1]\r\n",
      " [12 -1 -1 -1]]\r\n",
      "Rank  1  recvcount:  [0 0 2 0]\r\n",
      "Rank  1  disprecv:  [0 0 0 2]\r\n",
      "Rank  1  single_unsorted_bucket:  [5 6]\r\n",
      "Rank  2  disprecv:  [0 2 2 3]\r\n",
      "Rank  2  single_unsorted_bucket:  [ 9 11 11  8  8]\r\n",
      "Rank  3  disprecv:  [0 2 3 3]\r\n",
      "Rank  3  single_unsorted_bucket:  [14 13 12 15]\r\n",
      "Rank  0  disprecv:  [0 0 3 4]\r\n",
      "Rank  0  single_unsorted_bucket:  [0 2 1 1 2]\r\n",
      "[ 0  1  1  2  2  5  6  8  8  9 11 11 12 13 14 15]\r\n"
     ]
    }
   ],
   "source": [
    "!chmod 755 codes/mpi4py/bucket2.py\n",
    "!module load gcc/5.3.0 openmpi/1.10.3; mpirun -np 4 codes/mpi4py/bucket2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <center> N-Body Problem </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** Fundamental settings for most, if not all, of computational simulation problems: **\n",
    "\n",
    "- Given a space\n",
    "- Given a group of entities whose activities are (often) bounded within this space\n",
    "- Given a set of equation that governs how these entities react to one another and to attributes of the containing space\n",
    "- Simulate how these reactions impact all entities and the entire space overtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Computation requires parallelization\n",
    "- Experimental spaces are simulated at massive scale (millions of entities)\n",
    "- Individual time steps are significantly smaller than the total simulation time. \n",
    "- Time complexity can be reduced by approximating a cluster of distant bodies as a single distant body with mass sited at the center of the mass of the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/07/mass-bodies.png\" width=\"700\"/> \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Barnes-Hut Algorithm (2-D)\n",
    "\n",
    "Start with whole region in which one square contains the bodies (or particles).\n",
    "- First, this cube is divided into four subregions.\n",
    "- If a subregion contains no particles, it is deleted from further consideration.\n",
    "- If a subregion contains one body, it is retained.\n",
    "- If a subregion contains more than one body, it is recursively divided until every subregion contains one body.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Create an quadtree – a tree with up to four edges from each node\n",
    "- The leaves represent cells each containing one body.\n",
    "- After the tree has been constructed, the total mass and center of mass of the subregion is stored at each node.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/07/barnes-hut.png\" width=\"700\"/> \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Orthogonal Recursive Bisection\n",
    "\n",
    "- First, a vertical line found that divides area into two areas each with equal number of bodies. \n",
    "- For each area, a horizontal line found that divides it into two areas, each with equal number of bodies. \n",
    "- Repeated as required. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <img src=\"pictures/07/orthogonal.png\" width=\"400\"/> \n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (Anaconda)",
   "language": "python",
   "name": "anaconda_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
